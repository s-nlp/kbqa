{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/workspace/kbqa/\")  # go to parent dir\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 08:44:54.778166: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-08 08:44:55.042228: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-08-08 08:44:55.661800: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-08-08 08:44:55.661885: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-08-08 08:44:55.661894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import jsonlines\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers.models.graphormer.collating_graphormer import GraphormerDataCollator\n",
    "from transformers import GraphormerForGraphClassification\n",
    "from transformers.models.graphormer.collating_graphormer import algos_graphormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = 't5-xl-ssm'\n",
    "train_bs = 64\n",
    "eval_bs = 64\n",
    "data_prep = False\n",
    "push_to_hub = True\n",
    "model_weights = \"/workspace/storage/subgraphs_reranking_results/t5-xl-ssm/results/clefourrier/graphormer-base-pcqm4mv2_mse/checkpoint-119500\"\n",
    "model_name = \"clefourrier/graphormer-base-pcqm4mv2\"\n",
    "num_epochs = 50\n",
    "model_save_name = f\"{model_name}_mse\" if not model_weights else model_weights.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path):\n",
    "    jsonl_reader = jsonlines.open(path)\n",
    "    jsonl_reader_list = list(jsonl_reader)\n",
    "    df = []\n",
    "    for line in tqdm(jsonl_reader_list):\n",
    "        df.append(line)\n",
    "    df = pd.DataFrame(df)\n",
    "    return df\n",
    "\n",
    "def preprocess_df(df):\n",
    "    # turn list of entities into string\n",
    "    df[\"answerEntity\"] = df[\"answerEntity\"].apply(lambda x: \", \".join(x))\n",
    "    df[\"questionEntity\"] = df[\"questionEntity\"].apply(lambda x: \", \".join(x))\n",
    "    df[\"groundTruthAnswerEntity\"] = df[\"groundTruthAnswerEntity\"].apply(\n",
    "        lambda x: \", \".join(x)\n",
    "    )\n",
    "    df[\"correct\"] = df.apply(\n",
    "        lambda x: x[\"answerEntity\"] in x[\"groundTruthAnswerEntity\"], axis=1\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_prep:\n",
    "    train_df = read_jsonl(\n",
    "        f\"/workspace/storage/new_subgraph_dataset/{dataset_type}/mintaka_train_labeled.jsonl\"\n",
    "    )\n",
    "    train_df = preprocess_df(train_df)\n",
    "    val_df = read_jsonl(\n",
    "        f\"/workspace/storage/new_subgraph_dataset/{dataset_type}/mintaka_validation_labeled.jsonl\"\n",
    "    )\n",
    "    val_df = preprocess_df(val_df)\n",
    "    test_df = read_jsonl(\n",
    "        f\"/workspace/storage/new_subgraph_dataset/{dataset_type}/mintaka_test_labeled.jsonl\"\n",
    "    )\n",
    "    test_df = preprocess_df(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_graph(graph_data, answer_entity, ground_truth_entity):\n",
    "    # Create an empty dictionary to store the transformed graph\n",
    "    transformed_graph = {}\n",
    "\n",
    "    # Extract 'nodes' and 'links' from the graph_data\n",
    "    nodes = graph_data['nodes']\n",
    "    links = graph_data['links']\n",
    "\n",
    "    # Calculate num_nodes\n",
    "    num_nodes = len(nodes)\n",
    "\n",
    "    # Calculate edge_index\n",
    "    edge_index = [[link['source'], link['target']] for link in links]\n",
    "    edge_index = list(zip(*edge_index))\n",
    "\n",
    "    # Check if \"answerEntity\" matches with \"groundTruthAnswerEntity\" to get the label (y)\n",
    "    y = 1.0 if answer_entity in ground_truth_entity else 0.0\n",
    "\n",
    "    # Calculate node_feat based on 'type' key\n",
    "    node_feat = []\n",
    "    for node in nodes:\n",
    "        if node['type'] == 'INTERNAL':\n",
    "            node_feat.append([1])\n",
    "        elif node['type'] == 'ANSWER_CANDIDATE_ENTITY':\n",
    "            node_feat.append([2])\n",
    "        elif node['type'] == 'QUESTIONS_ENTITY':\n",
    "            node_feat.append([3])\n",
    "    \n",
    "    # Store the calculated values in the transformed_graph dictionary\n",
    "    transformed_graph['edge_index'] = edge_index\n",
    "    transformed_graph['num_nodes'] = num_nodes\n",
    "    transformed_graph['y'] = [y]\n",
    "    transformed_graph['node_feat'] = node_feat\n",
    "    transformed_graph['edge_attr'] = [[0]]\n",
    "\n",
    "    return transformed_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(edge_list):\n",
    "    # Find the maximum node ID in the edge_list\n",
    "    max_node_id = max(max(edge_list[0]), max(edge_list[1]))\n",
    "\n",
    "    # Initialize an empty adjacency matrix with zeros\n",
    "    adjacency_matrix = np.zeros((max_node_id+1, max_node_id+1), dtype=np.int32)  \n",
    "\n",
    "    # Add edges to the adjacency matrix\n",
    "    for src, dest in zip(edge_list[0], edge_list[1]):\n",
    "        adjacency_matrix[src, dest] = 1  \n",
    "    \n",
    "\n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(item):\n",
    "    \"\"\"Convert to the required format for Graphormer\"\"\"\n",
    "    attn_edge_type = None  # Initialize outside the loop\n",
    "\n",
    "    # Calculate adjacency matrix\n",
    "    adj = create_adjacency_matrix(item[\"edge_index\"])\n",
    "\n",
    "    shortest_path_result, path = algos_graphormer.floyd_warshall(adj)\n",
    "\n",
    "    try:\n",
    "        # Calculate max_dist and input_edges if the function call succeeds\n",
    "        shortest_path_result, path = algos_graphormer.floyd_warshall(adj)\n",
    "        max_dist = np.amax(shortest_path_result)\n",
    "        attn_edge_type = np.zeros((item[\"num_nodes\"], item[\"num_nodes\"], len(item['edge_attr'])), dtype=np.int64)\n",
    "        input_edges = algos_graphormer.gen_edge_input(max_dist, path, attn_edge_type)\n",
    "    except:\n",
    "        # If the function call fails, handle the exception\n",
    "        max_dist = 0\n",
    "        attn_edge_type = None\n",
    "        input_edges = np.zeros((item[\"num_nodes\"], item[\"num_nodes\"], max_dist, len(item['edge_attr'])), dtype=np.int64)\n",
    "        shortest_path_result = None\n",
    "\n",
    "    if attn_edge_type is None:\n",
    "        # Initialize attn_edge_type here if it hasn't been initialized already\n",
    "        attn_edge_type = np.zeros((item[\"num_nodes\"], item[\"num_nodes\"], len(item['edge_attr'])), dtype=np.int64)\n",
    "\n",
    "    # Set values for all the keys\n",
    "    processed_item = {\n",
    "        \"edge_index\": np.array(item[\"edge_index\"]),\n",
    "        \"num_nodes\": item[\"num_nodes\"],\n",
    "        \"y\": item[\"y\"],\n",
    "        \"node_feat\": np.array(item[\"node_feat\"]),\n",
    "        \"input_nodes\": np.array(item[\"node_feat\"]),  # Use node_feat as input_nodes if node_feat is the feature representation\n",
    "        \"edge_attr\": np.array(item[\"edge_attr\"]),\n",
    "        \"attn_bias\": np.zeros((item[\"num_nodes\"] + 1, item[\"num_nodes\"] + 1), dtype=np.single),\n",
    "        \"attn_edge_type\": attn_edge_type,\n",
    "        \"spatial_pos\": shortest_path_result.astype(np.int64) + 1,\n",
    "        \"in_degree\": np.sum(adj, axis=1).reshape(-1) + 1,\n",
    "        \"out_degree\": np.sum(adj, axis=1).reshape(-1) + 1,  # for undirected graph\n",
    "        \"input_edges\": input_edges + 1,\n",
    "        \"labels\": item.get(\"labels\", item[\"y\"]),  # Assuming \"labels\" key may or may not exist in the input data\n",
    "    }\n",
    "\n",
    "    return processed_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, save_path):\n",
    "    transformed_graph_dicts = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Transforming graphs\"):\n",
    "        try:\n",
    "            curr_dict = {}\n",
    "            graph_data = row['graph']\n",
    "            curr_dict['original_graph'] = graph_data\n",
    "\n",
    "            transformed_graph = transform_graph(graph_data, row['answerEntity'], row['groundTruthAnswerEntity'])\n",
    "            if len(transformed_graph[\"edge_index\"][0]) or len(transformed_graph[\"edge_index\"][1]) > 1:\n",
    "                curr_dict['question'] = row['question']\n",
    "                curr_dict['answerEntity'] = row['answerEntity']\n",
    "                curr_dict['groundTruthAnswerEntity'] = row['groundTruthAnswerEntity']\n",
    "                curr_dict['correct'] = row['correct']\n",
    "                curr_dict['transformed_graph'] = transformed_graph\n",
    "                transformed_graph_dicts.append(curr_dict)\n",
    "        except:\n",
    "            continue \n",
    "\n",
    "            \n",
    "    with open(save_path, 'w+') as file:\n",
    "        for transformed_graph in transformed_graph_dicts:\n",
    "            file.write(json.dumps(transformed_graph) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out yesno and count questions\n",
    "if data_prep:\n",
    "    train_df = pd.concat([train_df, val_df])\n",
    "    train_df = train_df[(train_df['complexityType'] != 'yesno') & (train_df['complexityType'] != 'count')] \n",
    "    test_df = test_df[(test_df['complexityType'] != 'yesno') & (test_df['complexityType'] != 'count')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trans_path = f'/workspace/storage/new_subgraph_dataset/{dataset_type}/graph_class/transformed_graphs_train_1.jsonl'\n",
    "test_trans_path = f'/workspace/storage/new_subgraph_dataset/{dataset_type}/graph_class/transformed_graphs_test_1.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_prep:\n",
    "    transform_data(test_df, test_trans_path)\n",
    "    transform_data(train_df, train_trans_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from unidecode import unidecode\n",
    "def try_literal_eval(s):\n",
    "    try:\n",
    "        return literal_eval(s)\n",
    "    except ValueError:\n",
    "        print('yo')\n",
    "        return s\n",
    "\n",
    "\n",
    "class CustomGraphDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                graph_dicts = json.loads(line)\n",
    "                preproc_graph = preprocess(graph_dicts['transformed_graph'])\n",
    "                \n",
    "                if preproc_graph['input_edges'].shape[2] != 0:\n",
    "                    self.data.append(preproc_graph)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Load your custom training and test datasets\n",
    "train_dataset = CustomGraphDataset(train_trans_path)\n",
    "test_dataset = CustomGraphDataset(test_trans_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79752"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "\n",
    "threshold = 0.5\n",
    "metric_classifier = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\", \"hyperml/balanced_accuracy\",])\n",
    "metric_regression = evaluate.combine([\"mae\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions[0]\n",
    "    results = metric_regression.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    predictions = predictions > threshold\n",
    "    results.update(\n",
    "        metric_classifier.compute(predictions=predictions, references=labels)\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/storage/subgraphs_reranking_results/t5-large-ssm/results/clefourrier/graphormer-base-pcqm4mv2_mse/checkpoint-62000'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": true,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00887918472290039,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "pytorch_model.bin",
       "rate": null,
       "total": 190778957,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833cbe13a8e245e2ac8f31a74e8ec5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/191M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": true,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019095659255981445,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload 1 LFS files",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f558f8926a449eb274b2812a8d2464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if  model_weights: # evaluating previous trained model weights\n",
    "    model = GraphormerForGraphClassification.from_pretrained(\n",
    "    model_weights,\n",
    "    num_classes=1,\n",
    "    ignore_mismatched_sizes=True,)\n",
    "    \n",
    "    # push this version to the hub\n",
    "    if push_to_hub:\n",
    "        model.push_to_hub(commit_message='previous trained best checkpoint', repo_id=f'hle2000/graphsormer_subgraphs_reranking_{dataset_type}')\n",
    "else: # training from scratch\n",
    "    model = GraphormerForGraphClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_classes=1,\n",
    "    ignore_mismatched_sizes=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "class CustomTrainer(Trainer):  \n",
    "    def get_labels(self):\n",
    "        labels = []\n",
    "        for i in self.train_dataset:\n",
    "            labels.append(int(i[\"y\"][0]))\n",
    "        return labels\n",
    "\n",
    "    def _get_train_sampler(self) -> torch.utils.data.Sampler:\n",
    "        labels = self.get_labels()\n",
    "        return self.create_sampler(labels)\n",
    "      \n",
    "    def create_sampler(self, target):\n",
    "        class_sample_count = np.array(\n",
    "            [len(np.where(target == t)[0]) for t in np.unique(target)]\n",
    "        )\n",
    "        weight = 1.0 / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in target])\n",
    "\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        samples_weigth = samples_weight.double()\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "        return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifiy the arguments for the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"/workspace/storage/subgraphs_reranking_results/{dataset_type}/results/{model_save_name}\",  # output directory\n",
    "    num_train_epochs=num_epochs,  # total number of training epochs\n",
    "    per_device_train_batch_size=train_bs,  # batch size per device during training\n",
    "    per_device_eval_batch_size=eval_bs,  # batch size for evaluation\n",
    "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    logging_dir=f\"/workspace/storage/subgraphs_reranking_results/{dataset_type}/logs/{model_save_name}\",  # directory for storing logs\n",
    "    load_best_model_at_end=True,  # load the best model when finished training (default metric is loss)\n",
    "    metric_for_best_model=\"balanced_accuracy\",  # select the base metrics\n",
    "    logging_steps=500,  # log & save weights each logging_steps\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",  # evaluate each `logging_steps`\n",
    "    report_to='wandb',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data collator\n",
    "data_collator = GraphormerDataCollator()\n",
    "# Initialize the Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,  # the callback that computes metrics of interest\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_weights: # training\n",
    "    train_results = trainer.train()\n",
    "    trainer.save_model(f\"/workspace/storage/subgraphs_reranking_results/{dataset_type}/results/{model_save_name}/best_checkpoint\")\n",
    "    if push_to_hub:\n",
    "        trainer.push_to_hub(commit_message='best checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='361' max='361' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [361/361 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhle2000\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/kbqa/experiments/subgraphs_reranking/graphormer/wandb/run-20230730_102605-ror4s59y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hle2000/huggingface/runs/ror4s59y' target=\"_blank\">scarlet-monkey-100</a></strong> to <a href='https://wandb.ai/hle2000/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hle2000/huggingface' target=\"_blank\">https://wandb.ai/hle2000/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hle2000/huggingface/runs/ror4s59y' target=\"_blank\">https://wandb.ai/hle2000/huggingface/runs/ror4s59y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.20189997553825378,\n",
       " 'eval_mae': 0.37012999221773013,\n",
       " 'eval_accuracy': 0.6751517779705117,\n",
       " 'eval_f1': 0.33501997336884154,\n",
       " 'eval_precision': 0.22059855038578444,\n",
       " 'eval_recall': 0.6960531169310218,\n",
       " 'eval_balanced_accuracy': 0.6842101547110266,\n",
       " 'eval_runtime': 46.0724,\n",
       " 'eval_samples_per_second': 500.517,\n",
       " 'eval_steps_per_second': 7.835}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_res = trainer.evaluate()\n",
    "evaluate_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23070/23070 [00:00<00:00, 2472722.92it/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = read_jsonl(test_trans_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_graph</th>\n",
       "      <th>question</th>\n",
       "      <th>answerEntity</th>\n",
       "      <th>groundTruthAnswerEntity</th>\n",
       "      <th>correct</th>\n",
       "      <th>transformed_graph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "      <td>Q893594</td>\n",
       "      <td>Q7245</td>\n",
       "      <td>False</td>\n",
       "      <td>{'edge_index': [[0, 1, 2, 2, 3, 3], [0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "      <td>Q102513</td>\n",
       "      <td>Q7245</td>\n",
       "      <td>False</td>\n",
       "      <td>{'edge_index': [[1, 1, 2, 2, 3, 4, 4], [0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "      <td>Q7245</td>\n",
       "      <td>Q7245</td>\n",
       "      <td>True</td>\n",
       "      <td>{'edge_index': [[1, 3, 3], [0, 0, 2]], 'num_no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "      <td>Q34652890</td>\n",
       "      <td>Q7245</td>\n",
       "      <td>False</td>\n",
       "      <td>{'edge_index': [[0, 1, 1, 2, 3, 3], [0, 0, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "      <td>Q5686</td>\n",
       "      <td>Q7245</td>\n",
       "      <td>False</td>\n",
       "      <td>{'edge_index': [[1, 3, 4, 4, 4], [0, 0, 0, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      original_graph  \\\n",
       "0  {'directed': True, 'multigraph': False, 'graph...   \n",
       "1  {'directed': True, 'multigraph': False, 'graph...   \n",
       "2  {'directed': True, 'multigraph': False, 'graph...   \n",
       "3  {'directed': True, 'multigraph': False, 'graph...   \n",
       "4  {'directed': True, 'multigraph': False, 'graph...   \n",
       "\n",
       "                                            question answerEntity  \\\n",
       "0  What man was a famous American author and also...      Q893594   \n",
       "1  What man was a famous American author and also...      Q102513   \n",
       "2  What man was a famous American author and also...        Q7245   \n",
       "3  What man was a famous American author and also...    Q34652890   \n",
       "4  What man was a famous American author and also...        Q5686   \n",
       "\n",
       "  groundTruthAnswerEntity  correct  \\\n",
       "0                   Q7245    False   \n",
       "1                   Q7245    False   \n",
       "2                   Q7245     True   \n",
       "3                   Q7245    False   \n",
       "4                   Q7245    False   \n",
       "\n",
       "                                   transformed_graph  \n",
       "0  {'edge_index': [[0, 1, 2, 2, 3, 3], [0, 0, 0, ...  \n",
       "1  {'edge_index': [[1, 1, 2, 2, 3, 4, 4], [0, 1, ...  \n",
       "2  {'edge_index': [[1, 3, 3], [0, 0, 2]], 'num_no...  \n",
       "3  {'edge_index': [[0, 1, 1, 2, 3, 3], [0, 0, 3, ...  \n",
       "4  {'edge_index': [[1, 3, 4, 4, 4], [0, 0, 0, 2, ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_csv = pd.read_csv(\n",
    "        f\"/workspace/storage/mintaka_seq2seq/{dataset_type}/test/results.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalGraphDataset(Dataset):\n",
    "    def __init__(self, is_corrects, graphs):\n",
    "        self.data = []\n",
    "        self.correct = []\n",
    "        for is_correct, graph in zip(is_corrects, graphs):\n",
    "            preproc_graph = preprocess(graph)\n",
    "            if preproc_graph['input_edges'].shape[2] != 0:\n",
    "                self.data.append(preproc_graph)\n",
    "                self.correct.append(is_correct)\n",
    "    \n",
    "    def get_new_correct(self):\n",
    "        return self.correct\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [00:25, 156.12it/s]\n"
     ]
    }
   ],
   "source": [
    "final_acc, top200_total, top1_total, seq2seq_correct = 0, 0, 0, 0\n",
    "    \n",
    "for idx, group in tqdm(res_csv.iterrows()):\n",
    "    curr_question_df = test_df[test_df[\"question\"] == group['question']]\n",
    "    if len(curr_question_df) == 0: # we don't have subgraph for this question, take answer from seq2seq\n",
    "        if group[\"answer_0\"] == group[\"target\"]:\n",
    "            seq2seq_correct += 1\n",
    "        else: # check if answer exist in 200 beams for question with no subgraphs\n",
    "            all_beams = group.tolist()[2:-1] # all 200 beams\n",
    "            all_beams = list(set(all_beams))\n",
    "            top200_total += 1 if group[\"target\"] in all_beams else 0\n",
    "            \n",
    "    else: # we have subgraph for this question  \n",
    "        all_beams = group.tolist()[2:-1] # all 200 beams\n",
    "        all_beams = list(set(all_beams))\n",
    "        \n",
    "        if group[\"target\"] not in all_beams: # no correct answer in beam\n",
    "            continue\n",
    "            \n",
    "        # correct answer exist in beam\n",
    "        top1_total += 1 if group[\"answer_0\"] == group[\"target\"] else 0\n",
    "        top200_total += 1\n",
    "        \n",
    "        transformed_graphs = curr_question_df[\"transformed_graph\"].tolist()\n",
    "        is_corrects = curr_question_df[\"correct\"].tolist()\n",
    "        current_dataset = EvalGraphDataset(is_corrects, transformed_graphs)\n",
    "        filtered_is_correct = current_dataset.get_new_correct()\n",
    "        \n",
    "        current_dataloader = torch.utils.data.DataLoader(current_dataset, \n",
    "                                                         batch_size=len(transformed_graphs), \n",
    "                                                         collate_fn=data_collator, \n",
    "                                                         shuffle=False)\n",
    "\n",
    "        # batch size should only be one\n",
    "        for item in current_dataloader:\n",
    "            logits = outputs = model(input_nodes = item['input_nodes'].to(device), \n",
    "                                    input_edges = item['input_edges'].to(device),\n",
    "                                    attn_bias = item['attn_bias'].to(device),\n",
    "                                    in_degree = item['in_degree'].to(device),\n",
    "                                    out_degree = item['out_degree'].to(device),\n",
    "                                    spatial_pos = item['spatial_pos'].to(device),\n",
    "                                    attn_edge_type = item['attn_edge_type'].to(device))\n",
    "            mse_pred = outputs.logits.flatten()\n",
    "            max_idx = mse_pred.argmax()\n",
    "        \n",
    "        if filtered_is_correct[max_idx]:\n",
    "            final_acc += 1 \n",
    "              \n",
    "\n",
    "# final rerankinga, top1 and top200 result\n",
    "reranking_res = (final_acc + seq2seq_correct)/ len(res_csv)\n",
    "top200 = (top200_total + seq2seq_correct)/len(res_csv)\n",
    "top1 = (top1_total + seq2seq_correct)/ len(res_csv)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25425, 0.22, 0.64375)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1, reranking_res, top200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
