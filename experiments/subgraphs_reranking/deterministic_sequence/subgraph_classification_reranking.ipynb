{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/workspace/kbqa/\")  # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 13:32:22.186424: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-15 13:32:22.385723: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-08-15 13:32:22.995457: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-08-15 13:32:22.995542: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-08-15 13:32:22.995550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import ujson\n",
    "import jsonlines\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting JSONL subgraphs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = 't5-xl-ssm'\n",
    "new_test_dataset = False \n",
    "train_bs = 16\n",
    "eval_bs = 32\n",
    "is_special_tok_context = True\n",
    "model_weights = None #\"hle2000/mpnet_subgraphs_reraking_t5xl\"\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_save_name = f\"{model_name}_mse_token_context\" if not model_weights else model_weights.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if model_weights: # evaluating\n",
    "    print('evaluating')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_weights)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_weights).to(\n",
    "        device\n",
    "    )\n",
    "else: # training from scratch\n",
    "    print('training from scratch')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": [\"[unused1]\", \"[unused2]\"]}\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1).to(\n",
    "        device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Mintaka_Subgraphs_T5_xl_ssm' if dataset_type == 't5-xl-ssm' else 'Mintaka_Subgraphs_T5_large_ssm'\n",
    "subgraphs_dataset = load_dataset(f'hle2000/{path}')\n",
    "train_df = subgraphs_dataset['train'].to_pandas()\n",
    "test_df = subgraphs_dataset['test'].to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting graph to its sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_names(subgraph, candidate_start_token=\"[unused1]\", candidate_end_token=\"[unused2]\",):\n",
    "    node_names = [subgraph.nodes[node][\"label\"] for node in subgraph.nodes()]\n",
    "    node_type = [subgraph.nodes[node][\"type\"] for node in subgraph.nodes()]\n",
    "    \n",
    "    if 'ANSWER_CANDIDATE_ENTITY' not in node_type:\n",
    "        return None\n",
    "\n",
    "    if is_special_tok_context:\n",
    "        candidate_idx = node_type.index(\"ANSWER_CANDIDATE_ENTITY\")\n",
    "        node_names[\n",
    "            candidate_idx\n",
    "        ] = f\"{candidate_start_token}{node_names[candidate_idx]}{candidate_end_token}\"\n",
    "    \n",
    "    return node_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_to_sequence(subgraph, node_names):\n",
    "    # getting adjency matrix and weight info\n",
    "    adj_matrix = nx.adjacency_matrix(subgraph).todense().tolist()\n",
    "    edge_data = subgraph.edges.data()\n",
    "\n",
    "    # adding our edge info\n",
    "    for edge in edge_data:\n",
    "        i, j, data = edge\n",
    "        i, j = int(i), int(j)\n",
    "        adj_matrix[i][j] = data[\"label\"]\n",
    "\n",
    "    sequence = []\n",
    "    # for adjency matrix, i, j means node i -> j\n",
    "    for i, row in enumerate(adj_matrix):\n",
    "        from_node = node_names[i]  # from node (node i)\n",
    "        for j, edge_info in enumerate(row):\n",
    "            to_node = node_names[j]\n",
    "            if edge_info != 0:  # no endge from_node -> to_node\n",
    "                sequence.extend([from_node, edge_info, to_node])\n",
    "    \n",
    "    sequence = \",\".join(str(node) for node in sequence)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "def try_literal_eval(s):\n",
    "    try:\n",
    "        return literal_eval(s)\n",
    "    except ValueError:\n",
    "        return s\n",
    "\n",
    "\n",
    "def get_sequences(df):\n",
    "    questions = list(df[\"question\"])\n",
    "    graphs = list(df[\"graph\"])\n",
    "    graph_seq = []\n",
    "    for question, graph in tqdm(zip(questions, graphs)):\n",
    "        graph_obj = nx.readwrite.json_graph.node_link_graph(try_literal_eval(graph))\n",
    "        try:\n",
    "            graph_node_names = get_node_names(graph_obj)\n",
    "            if graph_node_names is None:\n",
    "                curr_seq = \"ERROR_NO_CANDIDATE\"\n",
    "            else:     \n",
    "                curr_seq = graph_to_sequence(graph_obj, graph_node_names)\n",
    "                if is_special_tok_context:\n",
    "                    curr_seq = f\"{question}{tokenizer.sep_token}{curr_seq}\"\n",
    "        except KeyError:\n",
    "            curr_seq = \"ERROR_NO_LABEL\"\n",
    "        except nx.NetworkXError:\n",
    "            curr_seq = \"ERROR_EMPTY_GRAPH\"\n",
    "        graph_seq.append(curr_seq)\n",
    "    \n",
    "    return graph_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # get the sequences\n",
    "    df_sequences = get_sequences(df)\n",
    "    df[\"graph_sequence\"] = df_sequences\n",
    "    \n",
    "    # filter out all invalid graphs\n",
    "    error_df = df[\n",
    "        (df[\"graph_sequence\"] == \"ERROR_EMPTY_GRAPH\")\n",
    "        | (df[\"graph_sequence\"] == \"ERROR_NO_LABEL\")\n",
    "        | (df[\"graph_sequence\"] == \"ERROR_NO_CANDIDATE\")\n",
    "    ]\n",
    "    df = df.drop(error_df.index)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87924it [00:28, 3128.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answerEntity</th>\n",
       "      <th>questionEntity</th>\n",
       "      <th>groundTruthAnswerEntity</th>\n",
       "      <th>complexityType</th>\n",
       "      <th>graph</th>\n",
       "      <th>correct</th>\n",
       "      <th>graph_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2723bb1b</td>\n",
       "      <td>Which actor was the star of Titanic and was bo...</td>\n",
       "      <td>Q100711983</td>\n",
       "      <td>Q44578, Q65</td>\n",
       "      <td>Q38111</td>\n",
       "      <td>intersection</td>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>False</td>\n",
       "      <td>Which actor was the star of Titanic and was bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a9011ddf</td>\n",
       "      <td>What is the seventh tallest mountain in North ...</td>\n",
       "      <td>Q130018</td>\n",
       "      <td>Q49</td>\n",
       "      <td>Q1153188</td>\n",
       "      <td>ordinal</td>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>False</td>\n",
       "      <td>What is the seventh tallest mountain in North ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>982450cf</td>\n",
       "      <td>Who is the youngest current US governor?</td>\n",
       "      <td>Q11673</td>\n",
       "      <td>Q889821</td>\n",
       "      <td>Q3105215</td>\n",
       "      <td>superlative</td>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>False</td>\n",
       "      <td>Who is the youngest current US governor?&lt;/s&gt;Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>982450cf</td>\n",
       "      <td>Who is the youngest current US governor?</td>\n",
       "      <td>Q30</td>\n",
       "      <td>Q889821</td>\n",
       "      <td>Q3105215</td>\n",
       "      <td>superlative</td>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>False</td>\n",
       "      <td>Who is the youngest current US governor?&lt;/s&gt;[u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>982450cf</td>\n",
       "      <td>Who is the youngest current US governor?</td>\n",
       "      <td>Q132050</td>\n",
       "      <td>Q889821</td>\n",
       "      <td>Q3105215</td>\n",
       "      <td>superlative</td>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>False</td>\n",
       "      <td>Who is the youngest current US governor?&lt;/s&gt;go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           question answerEntity  \\\n",
       "0  2723bb1b  Which actor was the star of Titanic and was bo...   Q100711983   \n",
       "1  a9011ddf  What is the seventh tallest mountain in North ...      Q130018   \n",
       "2  982450cf           Who is the youngest current US governor?       Q11673   \n",
       "3  982450cf           Who is the youngest current US governor?          Q30   \n",
       "4  982450cf           Who is the youngest current US governor?      Q132050   \n",
       "\n",
       "  questionEntity groundTruthAnswerEntity complexityType  \\\n",
       "0    Q44578, Q65                  Q38111   intersection   \n",
       "1            Q49                Q1153188        ordinal   \n",
       "2        Q889821                Q3105215    superlative   \n",
       "3        Q889821                Q3105215    superlative   \n",
       "4        Q889821                Q3105215    superlative   \n",
       "\n",
       "                                               graph  correct  \\\n",
       "0  {'directed': True, 'multigraph': False, 'graph...    False   \n",
       "1  {'directed': True, 'multigraph': False, 'graph...    False   \n",
       "2  {'directed': True, 'multigraph': False, 'graph...    False   \n",
       "3  {'directed': True, 'multigraph': False, 'graph...    False   \n",
       "4  {'directed': True, 'multigraph': False, 'graph...    False   \n",
       "\n",
       "                                      graph_sequence  \n",
       "0  Which actor was the star of Titanic and was bo...  \n",
       "1  What is the seventh tallest mountain in North ...  \n",
       "2  Who is the youngest current US governor?</s>Un...  \n",
       "3  Who is the youngest current US governor?</s>[u...  \n",
       "4  Who is the youngest current US governor?</s>go...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = preprocess_data(train_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21967it [00:06, 3157.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answerEntity</th>\n",
       "      <th>questionEntity</th>\n",
       "      <th>groundTruthAnswerEntity</th>\n",
       "      <th>complexityType</th>\n",
       "      <th>graph</th>\n",
       "      <th>correct</th>\n",
       "      <th>graph_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fae46b21</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "      <td>Q191050</td>\n",
       "      <td>Q1497, Q846570</td>\n",
       "      <td>Q7245</td>\n",
       "      <td>intersection</td>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>False</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fae46b21</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "      <td>Q3259878</td>\n",
       "      <td>Q1497, Q846570</td>\n",
       "      <td>Q7245</td>\n",
       "      <td>intersection</td>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>False</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fae46b21</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "      <td>Q7245</td>\n",
       "      <td>Q1497, Q846570</td>\n",
       "      <td>Q7245</td>\n",
       "      <td>intersection</td>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>True</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fae46b21</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "      <td>Q1074614</td>\n",
       "      <td>Q1497, Q846570</td>\n",
       "      <td>Q7245</td>\n",
       "      <td>intersection</td>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>False</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fae46b21</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "      <td>Q15133865</td>\n",
       "      <td>Q1497, Q846570</td>\n",
       "      <td>Q7245</td>\n",
       "      <td>intersection</td>\n",
       "      <td>{'directed': True, 'multigraph': False, 'graph...</td>\n",
       "      <td>False</td>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           question answerEntity  \\\n",
       "0  fae46b21  What man was a famous American author and also...      Q191050   \n",
       "1  fae46b21  What man was a famous American author and also...     Q3259878   \n",
       "2  fae46b21  What man was a famous American author and also...        Q7245   \n",
       "3  fae46b21  What man was a famous American author and also...     Q1074614   \n",
       "4  fae46b21  What man was a famous American author and also...    Q15133865   \n",
       "\n",
       "   questionEntity groundTruthAnswerEntity complexityType  \\\n",
       "0  Q1497, Q846570                   Q7245   intersection   \n",
       "1  Q1497, Q846570                   Q7245   intersection   \n",
       "2  Q1497, Q846570                   Q7245   intersection   \n",
       "3  Q1497, Q846570                   Q7245   intersection   \n",
       "4  Q1497, Q846570                   Q7245   intersection   \n",
       "\n",
       "                                               graph  correct  \\\n",
       "0  {'directed': True, 'multigraph': False, 'graph...    False   \n",
       "1  {'directed': True, 'multigraph': False, 'graph...    False   \n",
       "2  {'directed': True, 'multigraph': False, 'graph...     True   \n",
       "3  {'directed': True, 'multigraph': False, 'graph...    False   \n",
       "4  {'directed': True, 'multigraph': False, 'graph...    False   \n",
       "\n",
       "                                      graph_sequence  \n",
       "0  What man was a famous American author and also...  \n",
       "1  What man was a famous American author and also...  \n",
       "2  What man was a famous American author and also...  \n",
       "3  What man was a famous American author and also...  \n",
       "4  What man was a famous American author and also...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = preprocess_data(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Dataset class for sequences \"\"\"\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        item = self.tokenizer(\n",
    "            row[\"graph_sequence\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item[\"input_ids\"] = item[\"input_ids\"].view(-1)\n",
    "        item[\"attention_mask\"] = item[\"attention_mask\"].view(-1)\n",
    "        item[\"labels\"] = torch.tensor(row[\"correct\"], dtype=torch.float) #pylint: disable=no-member\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataframe.index.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SequenceDataset(train_df, tokenizer)\n",
    "test_dataset = SequenceDataset(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "threshold = 0.5\n",
    "metric_classifier = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\", \"hyperml/balanced_accuracy\",])\n",
    "metric_regression = evaluate.combine([\"mae\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    results = metric_regression.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    predictions = predictions > threshold\n",
    "    results.update(\n",
    "        metric_classifier.compute(predictions=predictions, references=labels)\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifiy the arguments for the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"tmp\",#f\"/workspace/storage/subgraphs_reranking_results/t5-xl-ssm/results/{model_save_name}\",  # output directory\n",
    "    num_train_epochs=5,  # total number of training epochs\n",
    "    per_device_train_batch_size=train_bs,  # batch size per device during training\n",
    "    per_device_eval_batch_size=eval_bs,  # batch size for evaluation\n",
    "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    load_best_model_at_end=True,  # load the best model when finished training (default metric is loss)\n",
    "    metric_for_best_model=\"balanced_accuracy\",  # select the base metrics\n",
    "    logging_steps=500,  # log & save weights each logging_steps\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",  # evaluate each `logging_steps`\n",
    "    #report_to='wandb',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"custom trainer with sampler\"\"\"\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"get labels from train dataset\"\"\"\n",
    "        labels = []\n",
    "        for i in self.train_dataset:\n",
    "            labels.append(int(i['labels'].cpu().detach().numpy()))\n",
    "        return labels\n",
    "\n",
    "    def _get_train_sampler(self) -> torch.utils.data.Sampler:\n",
    "        \"\"\"create our custom sampler\"\"\"\n",
    "        labels = self.get_labels()\n",
    "        return self.create_sampler(labels)\n",
    "\n",
    "    def create_sampler(self, target):\n",
    "        \"\"\"weighted random sampler\"\"\"\n",
    "        class_sample_count = np.array(\n",
    "            [len(np.where(target == t)[0]) for t in np.unique(target)]\n",
    "        )\n",
    "        weight = 1.0 / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in target])\n",
    "\n",
    "        samples_weight = torch.from_numpy(samples_weight) #pylint: disable=no-member\n",
    "        samples_weight = samples_weight.double()\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "        return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,  # the instantiated Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=train_dataset,  # training dataset\n",
    "    eval_dataset=test_dataset,  # evaluation dataset\n",
    "    compute_metrics=compute_metrics,  # the callback that computes metrics of interest\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhle2000\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/kbqa/experiments/subgraphs_reranking/deterministic_sequence/wandb/run-20230815_133425-up18vgdh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hle2000/huggingface/runs/up18vgdh' target=\"_blank\">earnest-feather-104</a></strong> to <a href='https://wandb.ai/hle2000/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hle2000/huggingface' target=\"_blank\">https://wandb.ai/hle2000/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hle2000/huggingface/runs/up18vgdh' target=\"_blank\">https://wandb.ai/hle2000/huggingface/runs/up18vgdh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='27095' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   14/27095 00:04 < 2:51:17, 2.63 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/kbqa/experiments/subgraphs_reranking/deterministic_sequence/subgraph_classification_reranking.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6861695f726572616e6b5f656e765f6f6666696369616c222c2273657474696e6773223a7b22636f6e74657874223a226e6c70325f31227d7d/workspace/kbqa/experiments/subgraphs_reranking/deterministic_sequence/subgraph_classification_reranking.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6861695f726572616e6b5f656e765f6f6666696369616c222c2273657474696e6773223a7b22636f6e74657874223a226e6c70325f31227d7d/workspace/kbqa/experiments/subgraphs_reranking/deterministic_sequence/subgraph_classification_reranking.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_weights: \u001b[39m# training\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6861695f726572616e6b5f656e765f6f6666696369616c222c2273657474696e6773223a7b22636f6e74657874223a226e6c70325f31227d7d/workspace/kbqa/experiments/subgraphs_reranking/deterministic_sequence/subgraph_classification_reranking.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2665\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2664\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2665\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2667\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/accelerate/accelerator.py:1853\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1852\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1853\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "if not model_weights: # training\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='677' max='677' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [677/677 02:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhle2000\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/kbqa/experiments/subgraphs_reranking/deterministic_sequence/wandb/run-20230815_095911-pl03szld</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hle2000/huggingface/runs/pl03szld' target=\"_blank\">fallen-star-102</a></strong> to <a href='https://wandb.ai/hle2000/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hle2000/huggingface' target=\"_blank\">https://wandb.ai/hle2000/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hle2000/huggingface/runs/pl03szld' target=\"_blank\">https://wandb.ai/hle2000/huggingface/runs/pl03szld</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.17064249515533447,\n",
       " 'eval_mae': 0.2365033678702549,\n",
       " 'eval_accuracy': 0.775058866983702,\n",
       " 'eval_f1': 0.4957565721382736,\n",
       " 'eval_precision': 0.34624837357235794,\n",
       " 'eval_recall': 0.8724954462659381,\n",
       " 'eval_balanced_accuracy': 0.816706642451992,\n",
       " 'eval_runtime': 166.9574,\n",
       " 'eval_samples_per_second': 129.728,\n",
       " 'eval_steps_per_second': 4.055}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_res = trainer.evaluate()\n",
    "evaluate_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [01:55, 34.53it/s] \n"
     ]
    }
   ],
   "source": [
    "if not new_test_dataset:\n",
    "    res_csv = pd.read_csv(\n",
    "        f\"/workspace/storage/mintaka_seq2seq/{dataset_type}/test/results.csv\"\n",
    "    )\n",
    "    final_acc, top200_total, top1_total, seq2seq_correct = 0, 0, 0, 0\n",
    "    \n",
    "    for idx, group in tqdm(res_csv.iterrows()):\n",
    "        curr_question_df = test_df[test_df[\"question\"] == group['question']]\n",
    "        if len(curr_question_df) == 0: # we don't have subgraph for this question, take answer from seq2seq\n",
    "            if group[\"answer_0\"] == group[\"target\"]:\n",
    "                seq2seq_correct += 1\n",
    "            else: # check if answer exist in 200 beams for question with no subgraphs\n",
    "                all_beams = group.tolist()[2:-1] # all 200 beams\n",
    "                all_beams = list(set(all_beams))\n",
    "                top200_total += 1 if group[\"target\"] in all_beams else 0\n",
    "                \n",
    "        else: # we have subgraph for this question\n",
    "            all_beams = group.tolist()[2:-1] # all 200 beams\n",
    "            all_beams = list(set(all_beams))\n",
    "            \n",
    "            if group[\"target\"] not in all_beams: # no correct answer in beam\n",
    "                continue\n",
    "            \n",
    "            # correct answer exist in beam\n",
    "            top1_total += 1 if group[\"answer_0\"] == group[\"target\"] else 0\n",
    "            top200_total += 1\n",
    "            \n",
    "            # reranking\n",
    "            seqs = curr_question_df[\"graph_sequence\"].tolist()\n",
    "            is_corrects = curr_question_df[\"correct\"].tolist()\n",
    "            \n",
    "            tok_seq = tokenizer(seqs, padding=\"max_length\",\n",
    "                                max_length=512,\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\",\n",
    "                                )\n",
    "            mask = tok_seq[\"attention_mask\"].to(device)\n",
    "            input_id = tok_seq[\"input_ids\"].squeeze(1).to(device)\n",
    "            output = model(input_id, mask).logits\n",
    "            output = torch.flatten(output)\n",
    "\n",
    "            max_idx = output.argmax(dim=0).item()\n",
    "\n",
    "            if is_corrects[max_idx] is True:\n",
    "                final_acc += 1\n",
    "    \n",
    "    # final rerankinga, top1 and top200 result\n",
    "    reranking_res = (final_acc + seq2seq_correct)/ len(res_csv)\n",
    "    top200 = (top200_total + seq2seq_correct)/len(res_csv)\n",
    "    top1 = (top1_total + seq2seq_correct)/ len(res_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path):\n",
    "    jsonl_reader = jsonlines.open(path)\n",
    "    jsonl_reader_list = list(jsonl_reader)\n",
    "    df = []\n",
    "    for line in tqdm(jsonl_reader_list):\n",
    "        df.append(line)\n",
    "    df = pd.DataFrame(df)\n",
    "    return df\n",
    "\n",
    "if new_test_dataset: # new_test_dataset: # reranking for new test dataset format\n",
    "    test_dataset_path = '/workspace/storage/new_subgraph_dataset/t5-xl-ssm/mintaka_test_labeled_new.jsonl'\n",
    "    new_test_df = read_jsonl(test_dataset_path)\n",
    "    new_test_df = preprocess_data(new_test_df)\n",
    "    new_test_df_group = new_test_df.groupby('question')\n",
    "    final_acc, top200_total = 0, 0\n",
    "\n",
    "    for question, row in new_test_df_group:\n",
    "        answers = row['answerEntity'].tolist()\n",
    "        ground_truth = row['groundTruthAnswerEntity'].tolist()[0]\n",
    "        if ground_truth in answers:\n",
    "            top200_total += 1\n",
    "        # reranking\n",
    "        seqs = row[\"graph_sequence\"].tolist()\n",
    "        is_corrects = row[\"correct\"].tolist()\n",
    "        \n",
    "        tok_seq = tokenizer(seqs, padding=\"max_length\",\n",
    "                            max_length=512,\n",
    "                            truncation=True,\n",
    "                            return_tensors=\"pt\",\n",
    "                            )\n",
    "        mask = tok_seq[\"attention_mask\"].to(device)\n",
    "        input_id = tok_seq[\"input_ids\"].squeeze(1).to(device)\n",
    "        output = model(input_id, mask).logits\n",
    "        output = torch.flatten(output)\n",
    "\n",
    "        max_idx = output.argmax(dim=0).item()\n",
    "\n",
    "        if is_corrects[max_idx] is True:\n",
    "            final_acc += 1   \n",
    "    \n",
    "    # final rerankinga, top1 and top200 result\n",
    "    reranking_res = final_acc / len(new_test_df_group)\n",
    "    top200 = top200_total/len(new_test_df_group)\n",
    "    top1 = 'Not available for new test dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the final result to txt file\n",
    "with open(\n",
    "    f\"/workspace/storage/subgraphs_reranking_results/{dataset_type}/results/{model_save_name}/final_results_seq2seq.txt\",\n",
    "    \"w+\",\n",
    ") as f:\n",
    "    f.write(f'original top1: {top1}, top200: {top200}\\n')\n",
    "    f.write(f\"Final reranking accuracy: {reranking_res}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"trainer.evaluate() result:\\n\")\n",
    "    for k, v in evaluate_res.items():\n",
    "        f.write(f\"{k}:{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
