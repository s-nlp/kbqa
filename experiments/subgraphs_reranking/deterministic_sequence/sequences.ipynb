{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/workspace/kbqa/\")  # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 10:47:49.014306: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-29 10:47:49.174566: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-29 10:47:49.754688: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-02-29 10:47:49.754778: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-02-29 10:47:49.754788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import ujson\n",
    "import jsonlines\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting JSONL subgraphs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"t5-xl-ssm\"\n",
    "new_test_dataset = False\n",
    "train_bs = 32\n",
    "eval_bs = 32\n",
    "is_special_tok_context = False\n",
    "proccess_data_from_scratch = False\n",
    "model_weights = \"/workspace/storage/misc/subgraphs_reranking_results/new_sequences/t5-xl-ssm/all-mpnet-base-v2_wrapped_updated_seqs_no_highlight/checkpoint-13500\"\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_save_name = f\"{model_name.split('/')[-1]}_wrapped_updated_seqs_highlight_{is_special_tok_context}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    }
   ],
   "source": [
    "if model_weights:  # evaluating\n",
    "    print(\"evaluating\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": [\"[unused1]\", \"[unused2]\"]}\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_weights).to(device)\n",
    "else:  # training from scratch\n",
    "    print(\"training from scratch\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": [\"[unused1]\", \"[unused2]\"]}\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=1\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "def add_new_seqs(path, df):\n",
    "    \"\"\"get the new seqs from yaml and add to df\"\"\"\n",
    "    with open(path, \"r\") as stream:\n",
    "        try:\n",
    "            new_seqs = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "    updated_seqs = []\n",
    "    for curr_seq in new_seqs[\"data\"]:\n",
    "        updated_seqs.append(curr_seq[\"predicted\"])\n",
    "    df[\"updated_sequence\"] = updated_seqs\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting graph to its sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_names(\n",
    "    subgraph,\n",
    "    candidate_start_token=\"[unused1]\",\n",
    "    candidate_end_token=\"[unused2]\",\n",
    "    highlight=False,\n",
    "):\n",
    "    node_names = [subgraph.nodes[node][\"label\"] for node in subgraph.nodes()]\n",
    "    node_type = [subgraph.nodes[node][\"type\"] for node in subgraph.nodes()]\n",
    "\n",
    "    if \"ANSWER_CANDIDATE_ENTITY\" not in node_type:\n",
    "        return None\n",
    "\n",
    "    if highlight:\n",
    "        candidate_idx = node_type.index(\"ANSWER_CANDIDATE_ENTITY\")\n",
    "        node_names[\n",
    "            candidate_idx\n",
    "        ] = f\"{candidate_start_token}{node_names[candidate_idx]}{candidate_end_token}\"\n",
    "\n",
    "    return node_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_to_sequence(subgraph, node_names):\n",
    "    # getting adjency matrix and weight info\n",
    "    adj_matrix = nx.adjacency_matrix(subgraph).todense().tolist()\n",
    "    edge_data = subgraph.edges.data()\n",
    "\n",
    "    # adding our edge info\n",
    "    for edge in edge_data:\n",
    "        i, j, data = edge\n",
    "        i, j = int(i), int(j)\n",
    "        adj_matrix[i][j] = data[\"label\"]\n",
    "\n",
    "    sequence = []\n",
    "    # for adjency matrix, i, j means node i -> j\n",
    "    for i, row in enumerate(adj_matrix):\n",
    "        from_node = node_names[i]  # from node (node i)\n",
    "        for j, edge_info in enumerate(row):\n",
    "            to_node = node_names[j]\n",
    "            if edge_info != 0:  # no endge from_node -> to_node\n",
    "                sequence.extend([from_node, edge_info, to_node])\n",
    "\n",
    "    sequence = \",\".join(str(node) for node in sequence)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "def try_literal_eval(s):\n",
    "    try:\n",
    "        return literal_eval(s)\n",
    "    except ValueError:\n",
    "        return s\n",
    "\n",
    "\n",
    "def get_sequences(df):\n",
    "    questions = list(df[\"question\"])\n",
    "    graphs = list(df[\"graph\"])\n",
    "    hl_graph_seq, no_hl_graph_seq = [], []\n",
    "    for question, graph in tqdm(zip(questions, graphs)):\n",
    "        graph_obj = nx.readwrite.json_graph.node_link_graph(try_literal_eval(graph))\n",
    "        try:\n",
    "            hl_graph_node_names = get_node_names(graph_obj, highlight=True)\n",
    "            hl_seq = graph_to_sequence(graph_obj, hl_graph_node_names)\n",
    "            \n",
    "            no_hl_graph_node_names = get_node_names(graph_obj, highlight=False)\n",
    "            no_hl_seq = graph_to_sequence(graph_obj, no_hl_graph_node_names)\n",
    "            \n",
    "            hl_seq = f\"{question}{tokenizer.sep_token}{hl_seq}\"\n",
    "            no_hl_seq = f\"{question}{tokenizer.sep_token}{no_hl_seq}\"\n",
    "\n",
    "        except KeyError:\n",
    "            hl_seq, no_hl_seq = None, None\n",
    "        except nx.NetworkXError:\n",
    "            hl_seq, no_hl_seq = None, None\n",
    "        hl_graph_seq.append(hl_seq)\n",
    "        no_hl_graph_seq.append(no_hl_seq)\n",
    "\n",
    "    return no_hl_graph_seq, hl_graph_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label(graph, wd_id):\n",
    "    \"\"\"find label of the wikidata id using graph\"\"\"\n",
    "    for node_id in graph.nodes:\n",
    "        node = graph.nodes[node_id]\n",
    "        if node[\"name_\"] == wd_id:\n",
    "            return node[\"label\"]\n",
    "    return f\"cannot find label for {wd_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_updated_sequences(\n",
    "    df, candidate_start_token=\"[unused1]\", candidate_end_token=\"[unused2]\"\n",
    "):\n",
    "    no_hl_seqs, hl_seqs = [], []\n",
    "    for _, group in tqdm(df.iterrows()):\n",
    "        graph_obj = nx.readwrite.json_graph.node_link_graph(\n",
    "            try_literal_eval(group[\"graph\"])\n",
    "        )\n",
    "        updated_seq = group[\"updated_sequence\"]\n",
    "\n",
    "        try:\n",
    "            ans_ent_label = find_label(graph_obj, group[\"answerEntity\"])\n",
    "            splits = updated_seq.split(ans_ent_label)\n",
    "            hl_seq = f\"{splits[0].strip()} {candidate_start_token}{ans_ent_label}{candidate_end_token} {splits[1].strip()}\"\n",
    "            hl_seq = f\"{group['question']}{tokenizer.sep_token}{hl_seq}\"\n",
    "            no_hl_seq = f\"{group['question']}{tokenizer.sep_token}{updated_seq}\"\n",
    "        except:\n",
    "            hl_seq, no_hl_seq = None, None\n",
    "        no_hl_seqs.append(no_hl_seq)\n",
    "        hl_seqs.append(hl_seq)\n",
    "    return no_hl_seqs, hl_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Filter all graphs without ANSWER_CANDIDATE_ENTITY\n",
    "    df = df[\n",
    "        df[\"graph\"].apply(lambda x: \"ANSWER_CANDIDATE_ENTITY\" in str(x))\n",
    "    ]\n",
    "    \n",
    "    print(\"getting updated sequences\")\n",
    "    no_hl_seqs, hl_seqs = preproc_updated_sequences(df)\n",
    "    df[\"highlighted_updated_sequence\"] = hl_seqs\n",
    "    df[\"no_highlighted_updated_sequence\"] = no_hl_seqs\n",
    "\n",
    "    # get the determ sequences\n",
    "    print(\"getting sequences\")\n",
    "    no_hl_graph_seq, hl_graph_seq = get_sequences(df)\n",
    "    df[\"highlighted_sequence\"] = hl_graph_seq\n",
    "    df[\"no_highlighted_sequence\"] = no_hl_graph_seq\n",
    "\n",
    "    # filter out all invalid graphs\n",
    "    print(\"filtering\")\n",
    "    df = df.dropna(subset=[\"highlighted_updated_sequence\", \"no_highlighted_updated_sequence\",\n",
    "                           \"highlighted_sequence\", \"no_highlighted_sequence\"])  \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "def upload_dataset(train, test, path=\"hle2000/Mintaka_Sequences_T5_xl_ssm\"):\n",
    "    ds = DatasetDict()\n",
    "\n",
    "    ds[\"train\"] = Dataset.from_pandas(train)\n",
    "    ds[\"test\"] = Dataset.from_pandas(test)\n",
    "    ds.push_to_hub(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/hle2000___parquet/hle2000--Mintaka_Sequences_T5-xl-ssm-4753c3af9c5ad7ad/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc1b4051cf44c83a4ee4c2145c24092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if proccess_data_from_scratch:\n",
    "    path = (\n",
    "        \"Mintaka_Subgraphs_T5_xl_ssm\"\n",
    "        if dataset_type == \"t5-xl-ssm\"\n",
    "        else \"Mintaka_Subgraphs_T5_large_ssm\"\n",
    "    )\n",
    "    subgraphs_dataset = load_dataset(f\"hle2000/{path}\")\n",
    "    train_df = subgraphs_dataset[\"train\"].to_pandas()\n",
    "    test_df = subgraphs_dataset[\"test\"].to_pandas()\n",
    "\n",
    "    train_df = add_new_seqs(\n",
    "        \"/workspace/storage/misc/train_results_mintaka_xl.yaml\", train_df\n",
    "    )\n",
    "    test_df = add_new_seqs(\"/workspace/storage/misc/test_results_mintaka.yaml\", test_df)\n",
    "\n",
    "    train_df = preprocess_data(train_df)\n",
    "    test_df = preprocess_data(test_df)\n",
    "else:\n",
    "    seq_dataset = load_dataset(\"hle2000/Mintaka_Sequences_T5-xl-ssm\")\n",
    "    train_df = seq_dataset[\"train\"].to_pandas()\n",
    "    test_df = seq_dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for sequences\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, seq_name):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_name = seq_name\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        item = self.tokenizer(\n",
    "            row[self.seq_name],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item[\"input_ids\"] = item[\"input_ids\"].view(-1)\n",
    "        item[\"attention_mask\"] = item[\"attention_mask\"].view(-1)\n",
    "        item[\"labels\"] = torch.tensor(\n",
    "            row[\"correct\"], dtype=torch.float\n",
    "        )  # pylint: disable=no-member\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataframe.index.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_type = (\n",
    "    \"highlighted_updated_sequence\"\n",
    "    if is_special_tok_context\n",
    "    else \"no_highlighted_updated_sequence\"\n",
    ")\n",
    "train_dataset = SequenceDataset(train_df, tokenizer, seq_type)\n",
    "test_dataset = SequenceDataset(test_df, tokenizer, seq_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "threshold = 0.5\n",
    "metric_classifier = evaluate.combine(\n",
    "    [\n",
    "        \"accuracy\",\n",
    "        \"f1\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"hyperml/balanced_accuracy\",\n",
    "    ]\n",
    ")\n",
    "metric_regression = evaluate.combine([\"mae\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    results = metric_regression.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    predictions = predictions > threshold\n",
    "    results.update(\n",
    "        metric_classifier.compute(predictions=predictions, references=labels)\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifiy the arguments for the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"/workspace/storage/misc/subgraphs_reranking_results/new_sequences/t5-xl-ssm/{model_save_name}\",  # output directory\n",
    "    num_train_epochs=5,  # total number of training epochs\n",
    "    per_device_train_batch_size=train_bs,  # batch size per device during training\n",
    "    per_device_eval_batch_size=eval_bs,  # batch size for evaluation\n",
    "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    load_best_model_at_end=True,  # load the best model when finished training (default metric is loss)\n",
    "    metric_for_best_model=\"balanced_accuracy\",  # select the base metrics\n",
    "    logging_steps=500,  # log & save weights each logging_steps\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",  # evaluate each `logging_steps`\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"custom trainer with sampler\"\"\"\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"get labels from train dataset\"\"\"\n",
    "        labels = []\n",
    "        for i in self.train_dataset:\n",
    "            labels.append(int(i[\"labels\"].cpu().detach().numpy()))\n",
    "        return labels\n",
    "\n",
    "    def _get_train_sampler(self) -> torch.utils.data.Sampler:\n",
    "        \"\"\"create our custom sampler\"\"\"\n",
    "        labels = self.get_labels()\n",
    "        return self.create_sampler(labels)\n",
    "\n",
    "    def create_sampler(self, target):\n",
    "        \"\"\"weighted random sampler\"\"\"\n",
    "        class_sample_count = np.array(\n",
    "            [len(np.where(target == t)[0]) for t in np.unique(target)]\n",
    "        )\n",
    "        weight = 1.0 / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in target])\n",
    "\n",
    "        samples_weight = torch.from_numpy(samples_weight)  # pylint: disable=no-member\n",
    "        samples_weight = samples_weight.double()\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "        return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,  # the instantiated Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=train_dataset,  # training dataset\n",
    "    eval_dataset=test_dataset,  # evaluation dataset\n",
    "    compute_metrics=compute_metrics,  # the callback that computes metrics of interest\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "if not model_weights:  # training\n",
    "    trainer.train()\n",
    "    \n",
    "    checkpoint_best_path = (\n",
    "        f\"/workspace/storage/misc/subgraphs_reranking_results/new_sequences/t5-xl-ssm/results/{model_save_name}/checkpoint-best\"\n",
    "    )\n",
    "    model.save_pretrained(checkpoint_best_path)\n",
    "    tokenizer.save_pretrained(checkpoint_best_path)\n",
    "\n",
    "    print(\"Model dumped to \", checkpoint_best_path)\n",
    "    print(\"\\nFinal evaluation:\\n\\n\", trainer.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 02:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhle2000\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/kbqa/experiments/subgraphs_reranking/deterministic_sequence/wandb/run-20240229_105037-dmycu712</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hle2000/huggingface/runs/dmycu712' target=\"_blank\">rare-morning-142</a></strong> to <a href='https://wandb.ai/hle2000/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hle2000/huggingface' target=\"_blank\">https://wandb.ai/hle2000/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hle2000/huggingface/runs/dmycu712' target=\"_blank\">https://wandb.ai/hle2000/huggingface/runs/dmycu712</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.12478556483983994,\n",
       " 'eval_mae': 0.14876305486096023,\n",
       " 'eval_accuracy': 0.8579425784281122,\n",
       " 'eval_f1': 0.5668413047152273,\n",
       " 'eval_precision': 0.4930902675683623,\n",
       " 'eval_recall': 0.6665341812400636,\n",
       " 'eval_balanced_accuracy': 0.7777473173365074,\n",
       " 'eval_runtime': 142.7337,\n",
       " 'eval_samples_per_second': 126.403,\n",
       " 'eval_steps_per_second': 3.951}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_res = trainer.evaluate()\n",
    "evaluate_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/hle2000___parquet/hle2000--Mintaka_T5_xl_ssm_outputs-9a78025ce7d9a549/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a573b9958b4a92a35dcf0356e5c400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>target</th>\n",
       "      <th>answer_0</th>\n",
       "      <th>answer_1</th>\n",
       "      <th>answer_2</th>\n",
       "      <th>answer_3</th>\n",
       "      <th>answer_4</th>\n",
       "      <th>answer_5</th>\n",
       "      <th>answer_6</th>\n",
       "      <th>answer_7</th>\n",
       "      <th>...</th>\n",
       "      <th>answer_192</th>\n",
       "      <th>answer_193</th>\n",
       "      <th>answer_194</th>\n",
       "      <th>answer_195</th>\n",
       "      <th>answer_196</th>\n",
       "      <th>answer_197</th>\n",
       "      <th>answer_198</th>\n",
       "      <th>answer_199</th>\n",
       "      <th>target_out_of_vocab</th>\n",
       "      <th>__index_level_0__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>...</td>\n",
       "      <td>Louisa May Alcott</td>\n",
       "      <td>Ambrose Bierce</td>\n",
       "      <td>Ishmael Lehman</td>\n",
       "      <td>Mark Twain, Natchez, Missouri</td>\n",
       "      <td>Mark Twain, Louisa</td>\n",
       "      <td>Ishmael Levy</td>\n",
       "      <td>Ishmael Beam</td>\n",
       "      <td>Mark Twain, Natchez, Mississippi</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many Academy Awards has Jake Gyllenhaal be...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who is older, The Weeknd or Drake?</td>\n",
       "      <td>Drake</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>...</td>\n",
       "      <td>The Weeknd (2017)</td>\n",
       "      <td>The Weeknd's oldest</td>\n",
       "      <td>The Weeknd is older than Drake</td>\n",
       "      <td>The Weeknd's</td>\n",
       "      <td>Dierks Bentley</td>\n",
       "      <td>The Weeknd\"</td>\n",
       "      <td>Drake &amp; The Weeknd</td>\n",
       "      <td>The Weeknd's age</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many children did Donald Trump have?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>61</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is the main hero in Final Fantasy IX named Kuja?</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Is it a Final Fantasy</td>\n",
       "      <td>Does it include Kuja?</td>\n",
       "      <td>Is it</td>\n",
       "      <td>Is he Kuja</td>\n",
       "      <td>Is it No</td>\n",
       "      <td>Y Yes</td>\n",
       "      <td>Is Kuja</td>\n",
       "      <td>Is he called Kuja</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question      target    answer_0  \\\n",
       "0  What man was a famous American author and also...  Mark Twain  Mark Twain   \n",
       "1  How many Academy Awards has Jake Gyllenhaal be...           1           3   \n",
       "2                 Who is older, The Weeknd or Drake?       Drake  The Weeknd   \n",
       "3           How many children did Donald Trump have?           5           5   \n",
       "4   Is the main hero in Final Fantasy IX named Kuja?          No         Yes   \n",
       "\n",
       "     answer_1    answer_2    answer_3    answer_4    answer_5    answer_6  \\\n",
       "0  Mark Twain  Mark Twain  Mark Twain  Mark Twain  Mark Twain  Mark Twain   \n",
       "1           2           3           2           3           2           3   \n",
       "2  The Weeknd  The Weeknd  The Weeknd  The Weeknd  The Weeknd  The Weeknd   \n",
       "3           5           3           5           3           5           3   \n",
       "4         Yes         Yes         Yes         Yes         Yes         Yes   \n",
       "\n",
       "     answer_7  ...             answer_192             answer_193  \\\n",
       "0  Mark Twain  ...      Louisa May Alcott         Ambrose Bierce   \n",
       "1           2  ...                     13                     12   \n",
       "2  The Weeknd  ...      The Weeknd (2017)    The Weeknd's oldest   \n",
       "3           5  ...                     24                      6   \n",
       "4         Yes  ...  Is it a Final Fantasy  Does it include Kuja?   \n",
       "\n",
       "                       answer_194                     answer_195  \\\n",
       "0                  Ishmael Lehman  Mark Twain, Natchez, Missouri   \n",
       "1                               8                             11   \n",
       "2  The Weeknd is older than Drake                   The Weeknd's   \n",
       "3                              76                             13   \n",
       "4                           Is it                     Is he Kuja   \n",
       "\n",
       "           answer_196    answer_197          answer_198  \\\n",
       "0  Mark Twain, Louisa  Ishmael Levy        Ishmael Beam   \n",
       "1                  10             6                   9   \n",
       "2      Dierks Bentley   The Weeknd\"  Drake & The Weeknd   \n",
       "3                  61           108                   0   \n",
       "4            Is it No         Y Yes             Is Kuja   \n",
       "\n",
       "                         answer_199 target_out_of_vocab __index_level_0__  \n",
       "0  Mark Twain, Natchez, Mississippi               False                 0  \n",
       "1                                13               False                 1  \n",
       "2                  The Weeknd's age               False                 2  \n",
       "3                                 8               False                 3  \n",
       "4                 Is he called Kuja               False                 4  \n",
       "\n",
       "[5 rows x 204 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "res_csv = load_dataset(\n",
    "    f\"hle2000/Mintaka_T5_xl_ssm_outputs\", verification_mode=\"no_checks\"\n",
    ")[\"test\"].to_pandas()\n",
    "res_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [01:37, 40.98it/s] \n"
     ]
    }
   ],
   "source": [
    "final_acc, top200_total, top1_total, seq2seq_correct = 0, 0, 0, 0\n",
    "\n",
    "for idx, group in tqdm(res_csv.iterrows()):\n",
    "    curr_question_df = test_df[test_df[\"question\"] == group[\"question\"]]\n",
    "    if (\n",
    "        len(curr_question_df) == 0\n",
    "    ):  # we don't have subgraph for this question, take answer from seq2seq\n",
    "        if group[\"answer_0\"] == group[\"target\"]:\n",
    "            seq2seq_correct += 1\n",
    "        else:  # check if answer exist in 200 beams for question with no subgraphs\n",
    "            all_beams = group.tolist()[2:-1]  # all 200 beams\n",
    "            all_beams = set(all_beams)\n",
    "            top200_total += 1 if group[\"target\"] in all_beams else 0\n",
    "\n",
    "    else:  # we have subgraph for this question\n",
    "        all_beams = group.tolist()[2:-1]  # all 200 beams\n",
    "        all_beams = set(all_beams)\n",
    "\n",
    "        if group[\"target\"] not in all_beams:  # no correct answer in beam\n",
    "            continue\n",
    "\n",
    "        # correct answer exist in beam\n",
    "        top1_total += 1 if group[\"answer_0\"] == group[\"target\"] else 0\n",
    "        top200_total += 1\n",
    "\n",
    "        # reranking\n",
    "        seqs = curr_question_df[seq_type].tolist()\n",
    "        # print(seqs)\n",
    "        is_corrects = curr_question_df[\"correct\"].astype(bool).tolist()\n",
    "\n",
    "        tok_seq = tokenizer(\n",
    "            seqs,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        mask = tok_seq[\"attention_mask\"].to(device)\n",
    "        input_id = tok_seq[\"input_ids\"].squeeze(1).to(device)\n",
    "        output = model(input_id, mask).logits\n",
    "        output = torch.flatten(output)\n",
    "        # print(output)\n",
    "        # print(is_corrects)\n",
    "\n",
    "        max_idx = output.argmax(dim=0).item()\n",
    "\n",
    "        if is_corrects[max_idx] is True:\n",
    "            final_acc += 1\n",
    "    # break\n",
    "\n",
    "# final rerankinga, top1 and top200 result\n",
    "reranking_res = (final_acc + seq2seq_correct) / len(res_csv)\n",
    "top200 = (top200_total + seq2seq_correct) / len(res_csv)\n",
    "top1 = (top1_total + seq2seq_correct) / len(res_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1: 0.31725, top200: 0.69025, reranking top1: 0.35225\n"
     ]
    }
   ],
   "source": [
    "print(f\"top1: {top1}, top200: {top200}, reranking top1: {reranking_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "# saving the final result to txt file\n",
    "with open(\n",
    "    f\"/workspace/storage/misc/subgraphs_reranking_results/new_sequences/{dataset_type}/{model_save_name}/final_results_seq2seq.txt\",\n",
    "    \"w+\",\n",
    ") as f:\n",
    "    f.write(f\"original top1: {top1}, top200: {top200}\\n\")\n",
    "    f.write(f\"Final reranking accuracy: {reranking_res}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"trainer.evaluate() result:\\n\")\n",
    "    for k, v in evaluate_res.items():\n",
    "        f.write(f\"{k}:{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
