{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/workspace/kbqa/\")  # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 10:30:24.915790: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-15 10:30:25.075661: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-15 10:30:25.639301: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-03-15 10:30:25.639379: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-03-15 10:30:25.639386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import ujson\n",
    "import jsonlines\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration\n",
    "\n",
    "In the cell below, you can configure what type of training. For instance, you can choose what type of sequence to train MPNET on (both, gap, or g2t); or what kind of dataset (either xl or large). The following options are valid:\n",
    "\n",
    "* `sequence_type`: choices=[both, gap, or g2t] where both means both gap and g2t as features\n",
    "* `is_special_tok_context`: to whether highlight the answer candidate in the sequence or not\n",
    "* `model_weights`: None or the provided model weight, if None, train from scratch. If loading the model weights, we are evaluating & reranking *Note*: since this is using HF interface to train, it is **highly** recommended to use the script to train instead of this notebook\n",
    "* `seq_ds_path`: either \"hle2000/Mintaka_Updated_Sequences_T5-large-ssm\" or \"hle2000/Mintaka_Updated_Sequences_T5-xl-ssm\"\n",
    "* `res_csv_path`: either \"hle2000/Mintaka_T5_large_ssm_outputs\" or \"hle2000/Mintaka_T5_xl_ssm_outputs\" (**please use the same dataset version for both `seq_ds_path` and `res_csv_path`**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bs = 32\n",
    "eval_bs = 32\n",
    "\n",
    "sequence_type = \"gap\"\n",
    "res_csv_path = \"hle2000/Mintaka_T5_large_ssm_outputs\"\n",
    "seq_ds_path = \"hle2000/Mintaka_Updated_Sequences_T5-large-ssm\"\n",
    "\n",
    "is_special_tok_context = False  # highlight ans cand or no\n",
    "model_weights = \"/workspace/storage/misc/subgraphs_reranking_results/gap/T5-large-ssm/gap_hl_false_cutoff_50/outputs/checkpoint-best\"\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_save_name = (\n",
    "    f\"{model_name.split('/')[-1]}_{sequence_type}_highlight_{is_special_tok_context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n"
     ]
    }
   ],
   "source": [
    "if model_weights:  # evaluating\n",
    "    print(\"evaluating\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": [\"[unused1]\", \"[unused2]\"]}\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_weights).to(device)\n",
    "else:  # training from scratch\n",
    "    print(\"training from scratch\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": [\"[unused1]\", \"[unused2]\"]}\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=1\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/hle2000___parquet/hle2000--Mintaka_Updated_Sequences_T5-large-ssm-245ee66053ade193/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014636edb1bc48afbebd5f5b99ea8f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reading the sequence dataset\n",
    "seq_dataset = load_dataset(seq_ds_path)\n",
    "train_df = seq_dataset[\"train\"].to_pandas()\n",
    "test_df = seq_dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for sequences\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, seq_name):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_name = seq_name\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        item = self.tokenizer(\n",
    "            row[self.seq_name],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item[\"input_ids\"] = item[\"input_ids\"].view(-1)\n",
    "        item[\"attention_mask\"] = item[\"attention_mask\"].view(-1)\n",
    "        item[\"labels\"] = torch.tensor(\n",
    "            row[\"correct\"], dtype=torch.float\n",
    "        )  # pylint: disable=no-member\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataframe.index.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_type = \"highlighted\" if is_special_tok_context else \"no_highlighted\"\n",
    "seq_type = f\"{hl_type}_{sequence_type}_sequence\"\n",
    "\n",
    "train_dataset = SequenceDataset(train_df, tokenizer, seq_type)\n",
    "test_dataset = SequenceDataset(test_df, tokenizer, seq_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "threshold = 0.5\n",
    "metric_classifier = evaluate.combine(\n",
    "    [\n",
    "        \"accuracy\",\n",
    "        \"f1\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"hyperml/balanced_accuracy\",\n",
    "    ]\n",
    ")\n",
    "metric_regression = evaluate.combine([\"mae\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    results = metric_regression.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    predictions = predictions > threshold\n",
    "    results.update(\n",
    "        metric_classifier.compute(predictions=predictions, references=labels)\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifiy the arguments for the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"/workspace/storage/misc/subgraphs_reranking_results/new_sequences/t5-xl-ssm/{model_save_name}\",  # output directory\n",
    "    num_train_epochs=5,  # total number of training epochs\n",
    "    per_device_train_batch_size=train_bs,  # batch size per device during training\n",
    "    per_device_eval_batch_size=eval_bs,  # batch size for evaluation\n",
    "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    load_best_model_at_end=True,  # load the best model when finished training (default metric is loss)\n",
    "    metric_for_best_model=\"balanced_accuracy\",  # select the base metrics\n",
    "    logging_steps=500,  # log & save weights each logging_steps\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",  # evaluate each `logging_steps`\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"custom trainer with sampler\"\"\"\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"get labels from train dataset\"\"\"\n",
    "        labels = []\n",
    "        for i in self.train_dataset:\n",
    "            labels.append(int(i[\"labels\"].cpu().detach().numpy()))\n",
    "        return labels\n",
    "\n",
    "    def _get_train_sampler(self) -> torch.utils.data.Sampler:\n",
    "        \"\"\"create our custom sampler\"\"\"\n",
    "        labels = self.get_labels()\n",
    "        return self.create_sampler(labels)\n",
    "\n",
    "    def create_sampler(self, target):\n",
    "        \"\"\"weighted random sampler\"\"\"\n",
    "        class_sample_count = np.array(\n",
    "            [len(np.where(target == t)[0]) for t in np.unique(target)]\n",
    "        )\n",
    "        weight = 1.0 / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in target])\n",
    "\n",
    "        samples_weight = torch.from_numpy(samples_weight)  # pylint: disable=no-member\n",
    "        samples_weight = samples_weight.double()\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "        return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,  # the instantiated Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=train_dataset,  # training dataset\n",
    "    eval_dataset=test_dataset,  # evaluation dataset\n",
    "    compute_metrics=compute_metrics,  # the callback that computes metrics of interest\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "if not model_weights:  # training\n",
    "    trainer.train()\n",
    "\n",
    "    checkpoint_best_path = f\"/workspace/storage/misc/subgraphs_reranking_results/new_sequences/t5-xl-ssm/results/{model_save_name}/checkpoint-best\"\n",
    "    model.save_pretrained(checkpoint_best_path)\n",
    "    tokenizer.save_pretrained(checkpoint_best_path)\n",
    "\n",
    "    print(\"Model dumped to \", checkpoint_best_path)\n",
    "    print(\"\\nFinal evaluation:\\n\\n\", trainer.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='518' max='518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [518/518 02:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.14575406908988953,\n",
       " 'eval_mae': 0.19223036955801073,\n",
       " 'eval_accuracy': 0.8229367760617761,\n",
       " 'eval_f1': 0.5437587439763718,\n",
       " 'eval_precision': 0.4227701232777375,\n",
       " 'eval_recall': 0.7617595818815331,\n",
       " 'eval_balanced_accuracy': 0.7972663455626153,\n",
       " 'eval_runtime': 130.9255,\n",
       " 'eval_samples_per_second': 126.606,\n",
       " 'eval_steps_per_second': 3.956}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_res = trainer.evaluate()\n",
    "evaluate_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/hle2000___parquet/hle2000--Mintaka_T5_large_ssm_outputs-f31d696c971a731a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde0ebd19adc4e6a956422baccc66a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>target</th>\n",
       "      <th>answer_0</th>\n",
       "      <th>answer_1</th>\n",
       "      <th>answer_2</th>\n",
       "      <th>answer_3</th>\n",
       "      <th>answer_4</th>\n",
       "      <th>answer_5</th>\n",
       "      <th>answer_6</th>\n",
       "      <th>answer_7</th>\n",
       "      <th>...</th>\n",
       "      <th>answer_192</th>\n",
       "      <th>answer_193</th>\n",
       "      <th>answer_194</th>\n",
       "      <th>answer_195</th>\n",
       "      <th>answer_196</th>\n",
       "      <th>answer_197</th>\n",
       "      <th>answer_198</th>\n",
       "      <th>answer_199</th>\n",
       "      <th>target_out_of_vocab</th>\n",
       "      <th>__index_level_0__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What man was a famous American author and also...</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Harriet Beecher Stowe</td>\n",
       "      <td>Charles Dickens</td>\n",
       "      <td>William Faulkner</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Harriet Beecher Stowe</td>\n",
       "      <td>H. G. Wells</td>\n",
       "      <td>...</td>\n",
       "      <td>Theodore Sturgeon</td>\n",
       "      <td>H. P. Lovecraft</td>\n",
       "      <td>Stephen Crane</td>\n",
       "      <td>Horatio Bottomley</td>\n",
       "      <td>William Faulkner</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Edgar Allan Poe.</td>\n",
       "      <td>Horatio Parker</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many Academy Awards has Jake Gyllenhaal be...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>One</td>\n",
       "      <td>13</td>\n",
       "      <td>128</td>\n",
       "      <td>215</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who is older, The Weeknd or Drake?</td>\n",
       "      <td>Drake</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>Drake</td>\n",
       "      <td>Drake</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>Drake</td>\n",
       "      <td>Drake</td>\n",
       "      <td>Drake</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>...</td>\n",
       "      <td>DJ Khaled</td>\n",
       "      <td>TWiG</td>\n",
       "      <td>Drake,</td>\n",
       "      <td>Weeknd</td>\n",
       "      <td>TWENTY</td>\n",
       "      <td>TWiT</td>\n",
       "      <td>Twice as old</td>\n",
       "      <td>The Weeknd,</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many children did Donald Trump have?</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4 children</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is the main hero in Final Fantasy IX named Kuja?</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>The Final Fantasy IX.</td>\n",
       "      <td>Is Kuja the Hero</td>\n",
       "      <td>The Answer Is No</td>\n",
       "      <td>Is Final Fantasy VIII</td>\n",
       "      <td>Yu Yu Hakuku</td>\n",
       "      <td>Yep, yes</td>\n",
       "      <td>YYYY</td>\n",
       "      <td>The Final Fantasy VII Final Fantasy</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question      target    answer_0  \\\n",
       "0  What man was a famous American author and also...  Mark Twain  Mark Twain   \n",
       "1  How many Academy Awards has Jake Gyllenhaal be...           1           1   \n",
       "2                 Who is older, The Weeknd or Drake?       Drake  The Weeknd   \n",
       "3           How many children did Donald Trump have?           5           2   \n",
       "4   Is the main hero in Final Fantasy IX named Kuja?          No         Yes   \n",
       "\n",
       "     answer_1               answer_2         answer_3          answer_4  \\\n",
       "0  Mark Twain  Harriet Beecher Stowe  Charles Dickens  William Faulkner   \n",
       "1           1                      1                1                 1   \n",
       "2       Drake                  Drake       The Weeknd             Drake   \n",
       "3           3                      2                3                 2   \n",
       "4         Yes                    Yes              Yes               Yes   \n",
       "\n",
       "     answer_5               answer_6     answer_7  ...             answer_192  \\\n",
       "0  Mark Twain  Harriet Beecher Stowe  H. G. Wells  ...      Theodore Sturgeon   \n",
       "1           1                      2            1  ...                     11   \n",
       "2       Drake                  Drake   The Weeknd  ...              DJ Khaled   \n",
       "3           3                      2            4  ...                      7   \n",
       "4         Yes                    Yes          Yes  ...  The Final Fantasy IX.   \n",
       "\n",
       "         answer_193        answer_194             answer_195  \\\n",
       "0   H. P. Lovecraft     Stephen Crane      Horatio Bottomley   \n",
       "1                12               One                     13   \n",
       "2              TWiG            Drake,                 Weeknd   \n",
       "3                 9        4 children                     11   \n",
       "4  Is Kuja the Hero  The Answer Is No  Is Final Fantasy VIII   \n",
       "\n",
       "         answer_196  answer_197        answer_198  \\\n",
       "0  William Faulkner  Mark Twain  Edgar Allan Poe.   \n",
       "1               128         215               128   \n",
       "2            TWENTY        TWiT      Twice as old   \n",
       "3                 8          10                12   \n",
       "4      Yu Yu Hakuku    Yep, yes              YYYY   \n",
       "\n",
       "                            answer_199 target_out_of_vocab __index_level_0__  \n",
       "0                       Horatio Parker               False                 0  \n",
       "1                                  128               False                 1  \n",
       "2                          The Weeknd,               False                 2  \n",
       "3                                   13               False                 3  \n",
       "4  The Final Fantasy VII Final Fantasy               False                 4  \n",
       "\n",
       "[5 rows x 204 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "res_csv = load_dataset(res_csv_path, verification_mode=\"no_checks\")[\"test\"].to_pandas()\n",
    "res_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no_highlighted_gap_sequence'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [01:20, 49.64it/s] \n"
     ]
    }
   ],
   "source": [
    "final_acc, top200_total, top1_total, seq2seq_correct = 0, 0, 0, 0\n",
    "\n",
    "for idx, group in tqdm(res_csv.iterrows()):\n",
    "    curr_question_df = test_df[test_df[\"question\"] == group[\"question\"]]\n",
    "    if (\n",
    "        len(curr_question_df) == 0\n",
    "    ):  # we don't have subgraph for this question, take answer from seq2seq\n",
    "        if group[\"answer_0\"] == group[\"target\"]:\n",
    "            seq2seq_correct += 1\n",
    "        else:  # check if answer exist in 200 beams for question with no subgraphs\n",
    "            all_beams = group.tolist()[2:-1]  # all 200 beams\n",
    "            all_beams = set(all_beams)\n",
    "            top200_total += 1 if group[\"target\"] in all_beams else 0\n",
    "\n",
    "    else:  # we have subgraph for this question\n",
    "        all_beams = group.tolist()[2:-1]  # all 200 beams\n",
    "        all_beams = set(all_beams)\n",
    "\n",
    "        if group[\"target\"] not in all_beams:  # no correct answer in beam\n",
    "            continue\n",
    "\n",
    "        # correct answer exist in beam\n",
    "        top1_total += 1 if group[\"answer_0\"] == group[\"target\"] else 0\n",
    "        top200_total += 1\n",
    "\n",
    "        # reranking\n",
    "        seqs = curr_question_df[seq_type].tolist()\n",
    "        # print(seqs)\n",
    "        is_corrects = curr_question_df[\"correct\"].astype(bool).tolist()\n",
    "\n",
    "        tok_seq = tokenizer(\n",
    "            seqs,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        mask = tok_seq[\"attention_mask\"].to(device)\n",
    "        input_id = tok_seq[\"input_ids\"].squeeze(1).to(device)\n",
    "        output = model(input_id, mask).logits\n",
    "        output = torch.flatten(output)\n",
    "\n",
    "        max_idx = output.argmax(dim=0).item()\n",
    "\n",
    "        if is_corrects[max_idx] is True:\n",
    "            final_acc += 1\n",
    "\n",
    "# final rerankinga, top1 and top200 result\n",
    "reranking_res = (final_acc + seq2seq_correct) / len(res_csv)\n",
    "top200 = (top200_total + seq2seq_correct) / len(res_csv)\n",
    "top1 = (top1_total + seq2seq_correct) / len(res_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1: 0.25425, top200: 0.64375, reranking top1: 0.28425\n"
     ]
    }
   ],
   "source": [
    "print(f\"top1: {top1}, top200: {top200}, reranking top1: {reranking_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
