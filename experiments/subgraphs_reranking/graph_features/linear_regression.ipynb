{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration\n",
    "\n",
    "Configuration for the training of linear regression. The following are the options:\n",
    "* `ds_type`: either T5-large-ssm or T5-xl-ssm. The graph features dataset and the seq2seq outputs dataset will be depended on the specified model from this argument. \n",
    "* `classification`: to whether use `LogisticRegression` (`classification=True`) or `LinearRegression` (`classification=False`)\n",
    "* `text_only`: to wheter use the textual feature (question+answer)\n",
    "* `graph_numerical_only`: to whether use the numerical graph/network features (num nodes, edges, etc.)\n",
    "* `sequence_type`: either None (no g2t sequence used) or the desired sequence (T5, Determ. or GAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = \"xl\"\n",
    "cache_path = \"/workspace/storage/misc/huggingface\"\n",
    "graph_feats_path = f\"hle2000/KGQA_T5-{ds_type}-ssm\"\n",
    "res_csv_path = f\"hle2000/Mintaka_T5_{ds_type}_ssm_outputs\"\n",
    "classification = True  # clf true = logreg, false = linreg\n",
    "text_only = False  # to add text features or not\n",
    "graph_numerical_only = False  # to add numerical graph features or not\n",
    "sequence_type = \"gap\"  # to add g2t seq or not (if yes, t5, gap or determ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/workspace/storage/misc/huggingface/hle2000___parquet/hle2000--KGQA_T5-xl-ssm-1744d8040d58562f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08611fc9500d4be5818760bb7a9560fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "graph_features_ds = load_dataset(graph_feats_path, cache_dir=cache_path)\n",
    "train_df = graph_features_ds[\"train\"].to_pandas()\n",
    "val_df = graph_features_ds[\"validation\"].to_pandas()\n",
    "test_df = graph_features_ds[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_cols will be the same for all approaches, hence the core\n",
    "core_cols = [\"id\", \"question\", \"question_answer\", \"answerEntity\", \"graph\", \"correct\"]\n",
    "text_features = [\n",
    "    \"question\",\n",
    "    \"question_answer\",\n",
    "]  # textual features that we always want to drop for training (used later for reranking)\n",
    "id_cols = [\n",
    "    \"graph\",\n",
    "    \"id\",\n",
    "    \"answerEntity\",\n",
    "]  # cols used to id (for reranking), but drop for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def get_numeric_cols(df):\n",
    "    \"\"\"return all dataframe's cols with numeric features\"\"\"\n",
    "    cols_numeric = []\n",
    "    for k, v in df.dtypes.to_dict().items():\n",
    "        if (v is np.dtype(\"int64\") or v is np.dtype(\"float64\")) and k != \"correct\":\n",
    "            cols_numeric.append(k)\n",
    "\n",
    "    return cols_numeric\n",
    "\n",
    "\n",
    "def apply_col_scale(df, scaler, col, split_type):\n",
    "    \"\"\"apply scaling to dataframne's col based on split\"\"\"\n",
    "    if not col:\n",
    "        return df\n",
    "    if split_type == \"train\":  # fit transform for train\n",
    "        df[col] = scaler.fit_transform(df[col])\n",
    "    else:  # transform for val & test\n",
    "        df[col] = scaler.transform(df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# get the numerica cols in train and test to scale\n",
    "numeric_cols = get_numeric_cols(train_df)\n",
    "\n",
    "if graph_numerical_only:  # add numeric features for df if needed\n",
    "    core_cols += numeric_cols\n",
    "\n",
    "train_df = apply_col_scale(train_df, min_max_scaler, numeric_cols, \"train\")\n",
    "val_df = apply_col_scale(val_df, min_max_scaler, numeric_cols, \"val\")\n",
    "test_df = apply_col_scale(test_df, min_max_scaler, numeric_cols, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_features(with_text, seq_type):\n",
    "    \"\"\"depends on the configuration above, return additonal embedding features\n",
    "    to add to the dataframe. Embedding features could include Ques+Ans and/or\n",
    "    g2t embeddings (both through MPNET embeddings)\"\"\"\n",
    "    added_core_cols, emb_feats = [], []\n",
    "\n",
    "    if with_text:\n",
    "        added_core_cols.append(\"question_answer_embedding\")\n",
    "        emb_feats.append(\"question_answer_embedding\")\n",
    "\n",
    "    # add G2T sequence if needed\n",
    "    if seq_type:\n",
    "        added_core_cols.append(f\"{sequence_type}_sequence_embedding\")\n",
    "        emb_feats.append(f\"{sequence_type}_sequence_embedding\")\n",
    "\n",
    "    # added_core_cols will be added to core_cols list, emb_feats will be proccessed later\n",
    "    return added_core_cols, emb_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_cols, embedding_features = get_embedding_features(text_only, sequence_type)\n",
    "\n",
    "# filter dataframe based on column only keep the specified columns\n",
    "core_cols += added_cols\n",
    "train_df = train_df[core_cols]\n",
    "val_df = val_df[core_cols]\n",
    "test_df = test_df[core_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_emb(df, em_type):\n",
    "    \"\"\"split embeddings into individual columns for linreg/logreg\"\"\"\n",
    "    embeddings = df[em_type].tolist()\n",
    "    emb_dict = {}\n",
    "    for emb in embeddings:\n",
    "        for i, val in enumerate(emb):\n",
    "            curr_key = f\"{em_type}_{i}\"\n",
    "            if curr_key not in emb_dict:\n",
    "                emb_dict[f\"{em_type}_{i}\"] = [val]\n",
    "            else:\n",
    "                emb_dict[f\"{em_type}_{i}\"].append(val)\n",
    "\n",
    "    # returning a dataframe of just embeddings split into indivudal column format\n",
    "    return pd.DataFrame(emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_processed_emb(df):\n",
    "    \"\"\"add all processed embeddings in row format to df\"\"\"\n",
    "    em_df_list = []\n",
    "    for em in embedding_features:\n",
    "        em_df_list.append(process_emb(df, em))\n",
    "\n",
    "    df = pd.concat([df] + em_df_list, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the embedding features, turn arr -> individual col. Original arr format will be deleted to train\n",
    "if len(embedding_features) > 0:\n",
    "    train_df = add_processed_emb(train_df)\n",
    "    val_df = add_processed_emb(val_df)\n",
    "    test_df = add_processed_emb(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Log/Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all text, embedding features (after processing), and id cols (used for reranking)\n",
    "X_train = train_df.drop(\n",
    "    text_features + embedding_features + id_cols + [\"correct\"], axis=1\n",
    ")\n",
    "y_train = train_df[\"correct\"].astype(float).tolist()\n",
    "X_test = val_df.drop(text_features + embedding_features + id_cols + [\"correct\"], axis=1)\n",
    "y_test = val_df[\"correct\"].astype(float).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# regr = linear_model.LinearRegression()\n",
    "if classification:\n",
    "    model = linear_model.LogisticRegression(class_weight=\"balanced\", max_iter=10000)\n",
    "else:\n",
    "    model = linear_model.LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/workspace/storage/misc/huggingface/hle2000___parquet/hle2000--Mintaka_T5_xl_ssm_outputs-059583fb07b6dc35/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfd7e5e08174bef8b0ae8242ffd43c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: mintaka/en\n",
      "Found cached dataset mintaka (/workspace/storage/misc/huggingface/AmazonScience___mintaka/en/1.0.0/bb35d95f07aed78fa590601245009c5f585efe909dbd4a8f2a4025ccf65bb11d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391c3ecf052543b2b140722fbf51fd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "seq2seq_outputs = load_dataset(\n",
    "    res_csv_path, verification_mode=\"no_checks\", cache_dir=cache_path\n",
    ")\n",
    "mintaka_ds = load_dataset(\"AmazonScience/mintaka\", cache_dir=cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq2seq = seq2seq_outputs[\"train\"].to_pandas()\n",
    "test_seq2seq = seq2seq_outputs[\"test\"].to_pandas()\n",
    "val_seq2seq = seq2seq_outputs[\"validation\"].to_pandas()\n",
    "\n",
    "mintaka_val = mintaka_ds[\"validation\"].to_pandas()\n",
    "mintaka_test = mintaka_ds[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label(graph, wd_id):\n",
    "    \"\"\"find label of the wikidata id using graph\"\"\"\n",
    "    for node_id in graph.nodes:\n",
    "        node = graph.nodes[node_id]\n",
    "        if node[\"name_\"] == wd_id:\n",
    "            return node[\"label\"]\n",
    "    return f\"cannot find label for {wd_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_reranked_answers(df, seq2seq_res, mintaka_df):\n",
    "    \"\"\"getting the reranked answers. Each question will have the following format\n",
    "    {\n",
    "        \"QuestionID\": \"ID1\",\n",
    "        \"RankedAnswers\": [\n",
    "            {\n",
    "                \"AnswerEntityID\": \"ID\",\n",
    "                \"AnswerString\": \"String of prediction\",\n",
    "                \"Score\": null\n",
    "            },\n",
    "            {\n",
    "                \"AnswerEntityID\": \"ID\",\n",
    "                \"AnswerString\": \"String of prediction\",\n",
    "                \"Score\": null\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    if \"tfidf_vector\" in df:\n",
    "        df = df.drop([\"tfidf_vector\"], axis=1)\n",
    "    dict_list = []\n",
    "\n",
    "    for idx, group in tqdm(seq2seq_res.iterrows()):\n",
    "        # get the corresponding subgraphs/graph features for this question\n",
    "        curr_question_df = df[df[\"question\"] == group[\"question\"]]\n",
    "        group_dict = group.to_dict()  # dict of the current entry from seq2seq outputs\n",
    "        # building the dict entry for this question, with the above format from function description:\n",
    "        curr_ques_dict = {}\n",
    "\n",
    "        # we don't have subgraph/graph features for this question, take answer from seq2seq\n",
    "        if len(curr_question_df) == 0:\n",
    "            # find the ques id from mintaka\n",
    "            curr_id = mintaka_df[mintaka_df[\"question\"] == group_dict[\"question\"]][\n",
    "                \"id\"\n",
    "            ].values[0]\n",
    "            curr_ques_dict[\"QuestionID\"] = curr_id\n",
    "            ans_cands = []  # hold list of dict for RankedAnswers\n",
    "            for k, v in group_dict.items():\n",
    "                if \"answer\" in k:\n",
    "                    ans_cand_dict = {}  # current dict for this ranked answer\n",
    "                    ans_cand_dict[\"AnswerEntityID\"] = None\n",
    "                    ans_cand_dict[\"AnswerString\"] = v\n",
    "                    ans_cand_dict[\"Score\"] = None\n",
    "                    ans_cands.append(ans_cand_dict)\n",
    "            curr_ques_dict[\"RankedAnswers\"] = ans_cands\n",
    "        else:  # we have subgraph for this question\n",
    "            ans_ents = curr_question_df[\"answerEntity\"].tolist()\n",
    "            ans_strs = [\n",
    "                q_a.split(\";\")[-1].strip()\n",
    "                for q_a in curr_question_df[\"question_answer\"].tolist()\n",
    "            ]\n",
    "            ans_ent_str_dict = dict(\n",
    "                zip(ans_ents, ans_strs)\n",
    "            )  # look up table for ent and its label\n",
    "\n",
    "            curr_ques_dict[\"QuestionID\"] = curr_question_df[\"id\"].tolist()[0]\n",
    "            # dropping features for prediction (similar to X_train and X_test above)\n",
    "            curr_question_df = curr_question_df.drop(\n",
    "                embedding_features + text_features + id_cols + [\"correct\"], axis=1\n",
    "            )\n",
    "\n",
    "            all_beams = set(group.tolist()[2:-1])  # all 200 beams\n",
    "            if group[\"target\"] not in all_beams:  # no correct answer in beam\n",
    "                continue\n",
    "            preds = model.predict(curr_question_df)\n",
    "\n",
    "            if classification:  # get the model's confidence for classification\n",
    "                idxs = np.where(preds == 1.0)[0]  # idxs of ans predicted as 1\n",
    "                if len(idxs) == 0:  # model predict all ans to be 0\n",
    "                    continue\n",
    "\n",
    "                probs = []\n",
    "                for idx in idxs:  # getting the probablity for each ans predicted as 1\n",
    "                    curr_row = X_test[idx : idx + 1]\n",
    "                    curr_prob = model.predict_proba(curr_row)[0][1]  # [1]\n",
    "                    probs.append(curr_prob)\n",
    "                # mapping each ent to its respective probablity/model's confidence\n",
    "                ans_preds = dict(zip(ans_ents, probs))\n",
    "            else:\n",
    "                ans_preds = dict(zip(ans_ents, preds))\n",
    "\n",
    "            # sorting the dict from highest model's confidence to lowest\n",
    "            sorted_ans_preds = dict(\n",
    "                sorted(ans_preds.items(), key=lambda item: item[1], reverse=True)\n",
    "            )\n",
    "            ans_cands = []  # hold list of dict for RankedAnswers\n",
    "            for k, v in sorted_ans_preds.items():\n",
    "                ans_cand_dict = {}  # current dict for this ranked answer\n",
    "                ans_cand_dict[f\"AnswerEntityID\"] = k\n",
    "                ans_cand_dict[f\"AnswerString\"] = ans_ent_str_dict[k]\n",
    "                ans_cand_dict[f\"Score\"] = v\n",
    "                ans_cands.append(ans_cand_dict)\n",
    "\n",
    "            curr_ques_dict[\"RankedAnswers\"] = ans_cands\n",
    "        dict_list.append(curr_ques_dict)\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def write_jsonl(file_path, data):\n",
    "    with open(file_path, \"w+\") as f:\n",
    "        for d in data:\n",
    "            json.dump(d, f)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:11, 168.29it/s]\n"
     ]
    }
   ],
   "source": [
    "model_type = \"logreg\" if classification else \"linreg\"\n",
    "val_result = get_reranked_answers(val_df, val_seq2seq, mintaka_val)\n",
    "path = f\"/workspace/storage/misc/features_reranking/lin_log/{model_type}_Gap_T5-{ds_type}_val_predictions.jsonl\"\n",
    "write_jsonl(path, val_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [00:14, 270.96it/s]\n"
     ]
    }
   ],
   "source": [
    "test_result = get_reranked_answers(test_df, test_seq2seq, mintaka_test)\n",
    "path = f\"/workspace/storage/misc/features_reranking/lin_log/{model_type}_Gap_T5-{ds_type}_test_predictions.jsonl\"\n",
    "write_jsonl(path, test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
