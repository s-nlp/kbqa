{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from pathlib import Path\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from typing import List, Optional, Dict\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pywikidata import Entity\n",
    "from kbqa.utils.train_eval import get_best_checkpoint_path\n",
    "\n",
    "from trie import MarisaTrie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to initialize NVML: Unknown Error\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'wikidata-simplequestions' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/askplatypus/wikidata-simplequestions.git\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    \"./wikidata-simplequestions/annotated_wd_data_test_answerable.txt\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"S\", \"P\", \"O\", \"Q\"],\n",
    ")\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "    \"./wikidata-simplequestions/annotated_wd_data_train_answerable.txt\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"S\", \"P\", \"O\", \"Q\"],\n",
    ")\n",
    "\n",
    "valid_df = pd.read_csv(\n",
    "    \"./wikidata-simplequestions/annotated_wd_data_valid_answerable.txt\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"S\", \"P\", \"O\", \"Q\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19481"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.index.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2438\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>P</th>\n",
       "      <th>O</th>\n",
       "      <th>Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q154335</td>\n",
       "      <td>P509</td>\n",
       "      <td>Q12152</td>\n",
       "      <td>what was the cause of death of yves klein?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q62498</td>\n",
       "      <td>P21</td>\n",
       "      <td>Q6581097</td>\n",
       "      <td>how does engelbert zaschka identify?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q182485</td>\n",
       "      <td>P413</td>\n",
       "      <td>Q1143358</td>\n",
       "      <td>what position does pee wee reese play in baseb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q12152</td>\n",
       "      <td>P509</td>\n",
       "      <td>Q6371569</td>\n",
       "      <td>Which Swiss conductor's cause of death is myoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q472382</td>\n",
       "      <td>P19</td>\n",
       "      <td>Q23051</td>\n",
       "      <td>what is the place of birth of sam edwards??</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         S     P         O                                                  Q\n",
       "0  Q154335  P509    Q12152         what was the cause of death of yves klein?\n",
       "1   Q62498   P21  Q6581097               how does engelbert zaschka identify?\n",
       "2  Q182485  P413  Q1143358  what position does pee wee reese play in baseb...\n",
       "3   Q12152  P509  Q6371569  Which Swiss conductor's cause of death is myoc...\n",
       "4  Q472382   P19    Q23051        what is the place of birth of sam edwards??"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "filtered_test = np.load(\"simple_questions_filtered.npy\")\n",
    "test_df = pd.DataFrame(filtered_test, columns=[\"S\", \"P\", \"O\", \"Q\"])\n",
    "print(test_df.index.size)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/t5-large-ssm\")\n",
    "\n",
    "def convert_to_features(example_batch):\n",
    "    input_encodings = tokenizer(\n",
    "        example_batch['Q'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "    )m\n",
    "    return input_encodings\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    get_best_checkpoint_path(Path('../../../runs/wdsq_tunned/google_t5-large-ssm/models/'))\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "num_return_sequences=200\n",
    "num_beams=200\n",
    "num_beam_groups=20\n",
    "diversity_penalty=0.1\n",
    "batch_size=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./wdsq_dataset/’: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 24.16ba/s]\n",
      "evaluate model:   0%|          | 0/9741 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:197: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  warnings.warn(\n",
      "2023-01-09 08:56:30.501473: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-09 08:56:30.667079: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-09 08:56:30.703443: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-09 08:56:31.333010: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-09 08:56:31.333088: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-09 08:56:31.333096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "evaluate model:   0%|          | 1/9741 [00:04<10:57:46,  4.05s/it]/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:197: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  warnings.warn(\n",
      "evaluate model:   0%|          | 14/9741 [00:16<3:13:06,  1.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     _df\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./wdsq_dataset/\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_no_prefix_tree.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     _df\u001b[39m.\u001b[39mto_pickle(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./wdsq_dataset/\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_no_prefix_tree.pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m prepare_dataset(train_df, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m prepare_dataset(valid_df, \u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m prepare_dataset(test_df, \u001b[39m'\u001b[39m\u001b[39mfiltered_test\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb Cell 6\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(df, name)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m generated_decoded \u001b[39m=\u001b[39m {\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39manswer_\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m: [] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_return_sequences)}\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(dataloader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mevaluate model\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     generated_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         batch[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         num_beams\u001b[39m=\u001b[39;49mnum_beams,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         num_return_sequences\u001b[39m=\u001b[39;49mnum_return_sequences,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         num_beam_groups\u001b[39m=\u001b[39;49mnum_beam_groups,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         diversity_penalty\u001b[39m=\u001b[39;49mdiversity_penalty,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m# prefix_allowed_tokens_fn=prefix_allowed_tokens,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     generated_decoded_batch \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         generated_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f7067222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/candidates_selection_experiments/generate_answers_with_prefix_tree.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     current_batch_size \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1444\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1441\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[1;32m   1443\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[0;32m-> 1444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup_beam_search(\n\u001b[1;32m   1445\u001b[0m         input_ids,\n\u001b[1;32m   1446\u001b[0m         beam_scorer,\n\u001b[1;32m   1447\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1448\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1449\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   1450\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   1451\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   1452\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1453\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1454\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1455\u001b[0m     )\n\u001b[1;32m   1457\u001b[0m \u001b[39melif\u001b[39;00m is_constraint_gen_mode:\n\u001b[1;32m   1458\u001b[0m     \u001b[39mif\u001b[39;00m num_return_sequences \u001b[39m>\u001b[39m num_beams:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:2914\u001b[0m, in \u001b[0;36mGenerationMixin.group_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m \u001b[39m# stateless\u001b[39;00m\n\u001b[1;32m   2913\u001b[0m process_beam_indices \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(beam_indices, ()) \u001b[39mif\u001b[39;00m beam_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2914\u001b[0m beam_outputs \u001b[39m=\u001b[39m beam_scorer\u001b[39m.\u001b[39;49mprocess(\n\u001b[1;32m   2915\u001b[0m     group_input_ids,\n\u001b[1;32m   2916\u001b[0m     next_token_scores,\n\u001b[1;32m   2917\u001b[0m     next_tokens,\n\u001b[1;32m   2918\u001b[0m     next_indices,\n\u001b[1;32m   2919\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   2920\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   2921\u001b[0m     beam_indices\u001b[39m=\u001b[39;49mprocess_beam_indices,\n\u001b[1;32m   2922\u001b[0m )\n\u001b[1;32m   2923\u001b[0m beam_scores[batch_group_indices] \u001b[39m=\u001b[39m beam_outputs[\u001b[39m\"\u001b[39m\u001b[39mnext_beam_scores\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2924\u001b[0m beam_next_tokens \u001b[39m=\u001b[39m beam_outputs[\u001b[39m\"\u001b[39m\u001b[39mnext_beam_tokens\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:255\u001b[0m, in \u001b[0;36mBeamSearchScorer.process\u001b[0;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices)\u001b[0m\n\u001b[1;32m    253\u001b[0m batch_beam_idx \u001b[39m=\u001b[39m batch_idx \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_size \u001b[39m+\u001b[39m next_index\n\u001b[1;32m    254\u001b[0m \u001b[39m# add to generated hypotheses if end of sentence\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m \u001b[39mif\u001b[39;00m (eos_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (next_token\u001b[39m.\u001b[39;49mitem() \u001b[39m==\u001b[39m eos_token_id):\n\u001b[1;32m    256\u001b[0m     \u001b[39m# if beam_token does not belong to top num_beams tokens, it should not be added\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     is_beam_token_worse_than_top_num_beams \u001b[39m=\u001b[39m beam_token_rank \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_size\n\u001b[1;32m    258\u001b[0m     \u001b[39mif\u001b[39;00m is_beam_token_worse_than_top_num_beams:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!mkdir ./wdsq_dataset/\n",
    "\n",
    "\n",
    "def prepare_dataset(df, name):\n",
    "    test_dataset = datasets.Dataset.from_pandas(df)\n",
    "    test_dataset = test_dataset.map(convert_to_features, batched=True)\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    generated_decoded = {f\"answer_{idx}\": [] for idx in range(num_return_sequences)}\n",
    "    for batch in tqdm(dataloader, desc=\"evaluate model\"):\n",
    "        generated_ids = model.generate(\n",
    "            batch[\"input_ids\"].to(device),\n",
    "            num_beams=num_beams,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beam_groups=num_beam_groups,\n",
    "            diversity_penalty=diversity_penalty,\n",
    "            # prefix_allowed_tokens_fn=prefix_allowed_tokens,\n",
    "        )\n",
    "        generated_decoded_batch = tokenizer.batch_decode(\n",
    "            generated_ids,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )\n",
    "\n",
    "        current_batch_size = batch[\"input_ids\"].shape[0]\n",
    "        for start_batch_idx in range(\n",
    "            0, current_batch_size * num_return_sequences, num_return_sequences\n",
    "        ):\n",
    "            for answer_idx, answer in enumerate(\n",
    "                generated_decoded_batch[\n",
    "                    start_batch_idx : start_batch_idx + num_return_sequences\n",
    "                ]\n",
    "            ):\n",
    "                generated_decoded[f\"answer_{answer_idx}\"].append(answer)\n",
    "\n",
    "    _df = pd.concat([df, pd.DataFrame(generated_decoded)], axis=1)\n",
    "    _df.to_csv(f\"./wdsq_dataset/{name}_no_prefix_tree.csv\", index=False)\n",
    "    _df.to_pickle(f\"./wdsq_dataset/{name}_no_prefix_tree.pkl\")\n",
    "\n",
    "\n",
    "prepare_dataset(train_df, \"train\")\n",
    "prepare_dataset(valid_df, \"valid\")\n",
    "prepare_dataset(test_df, \"filtered_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./wdsq_t5_trie.pkl', 'rb') as f:\n",
    "#     trie = pickle.load(f)\n",
    "\n",
    "# def prefix_allowed_tokens(batch_id, sent):\n",
    "#     result = trie.get(sent.tolist())\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: destination path 'wikidata_rubq' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikidata_rubq (./wikidata_rubq/multiple_en/0.0.1/8b87b3d5a6c617afe07b22dd53456e86d924379154f567767edfc8653ae0516d)\n",
      "100%|██████████| 2/2 [00:00<00:00, 641.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 42.30ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 32.05ba/s]\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/s-nlp/wikidata_rubq.git\n",
    "test_dataset = datasets.load_dataset(\n",
    "    \"wikidata_rubq\", \"multiple_en\", cache_dir=\".\", ignore_verifications=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/t5-large-ssm\")\n",
    "\n",
    "\n",
    "def convert_to_features(example_batch):\n",
    "    input_encodings = tokenizer(\n",
    "        example_batch[\"question\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "    )\n",
    "    return input_encodings\n",
    "\n",
    "\n",
    "test_dataset = test_dataset.map(convert_to_features, batched=True)[\"test\"]\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluate model:   0%|          | 0/700 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:197: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  warnings.warn(\n",
      "2023-01-08 14:38:01.302002: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-08 14:38:01.461366: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-08 14:38:01.494898: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-08 14:38:02.264650: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-08 14:38:02.264723: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-08 14:38:02.264732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "evaluate model:   0%|          | 1/700 [00:04<51:55,  4.46s/it]/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:197: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  warnings.warn(\n",
      "evaluate model: 100%|██████████| 700/700 [11:36<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    get_best_checkpoint_path(\n",
    "        Path(\"../../../runs/wdsq_tunned/google_t5-large-ssm/models/\")\n",
    "    )\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "num_return_sequences = 200\n",
    "num_beams = 200\n",
    "num_beam_groups = 20\n",
    "diversity_penalty = 0.1\n",
    "batch_size = 2\n",
    "\n",
    "dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "generated_decoded = {f\"answer_{idx}\": [] for idx in range(num_return_sequences)}\n",
    "for batch in tqdm(dataloader, desc=\"evaluate model\"):\n",
    "    generated_ids = model.generate(\n",
    "        batch[\"input_ids\"].to(device),\n",
    "        num_beams=num_beams,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        num_beam_groups=num_beam_groups,\n",
    "        diversity_penalty=diversity_penalty,\n",
    "        # prefix_allowed_tokens_fn=prefix_allowed_tokens,\n",
    "    )\n",
    "    generated_decoded_batch = tokenizer.batch_decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "    )\n",
    "\n",
    "    current_batch_size = batch[\"input_ids\"].shape[0]\n",
    "    for start_batch_idx in range(\n",
    "        0, current_batch_size * num_return_sequences, num_return_sequences\n",
    "    ):\n",
    "        for answer_idx, answer in enumerate(\n",
    "            generated_decoded_batch[\n",
    "                start_batch_idx : start_batch_idx + num_return_sequences\n",
    "            ]\n",
    "        ):\n",
    "            generated_decoded[f\"answer_{answer_idx}\"].append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_with_answers_df = pd.concat([test_df, pd.DataFrame(generated_decoded)], axis=1)\n",
    "# test_with_answers_df.to_csv('rubq_test_with_answers_no_prefix.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>Q</th>\n",
       "      <th>answer_0</th>\n",
       "      <th>answer_1</th>\n",
       "      <th>answer_2</th>\n",
       "      <th>answer_3</th>\n",
       "      <th>answer_4</th>\n",
       "      <th>answer_5</th>\n",
       "      <th>answer_6</th>\n",
       "      <th>answer_7</th>\n",
       "      <th>...</th>\n",
       "      <th>answer_190</th>\n",
       "      <th>answer_191</th>\n",
       "      <th>answer_192</th>\n",
       "      <th>answer_193</th>\n",
       "      <th>answer_194</th>\n",
       "      <th>answer_195</th>\n",
       "      <th>answer_196</th>\n",
       "      <th>answer_197</th>\n",
       "      <th>answer_198</th>\n",
       "      <th>answer_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Q7944, Q167903, Q5975740, Q60186, Q2580904, Q...</td>\n",
       "      <td>What can cause a tsunami?</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>...</td>\n",
       "      <td>actinopteryx</td>\n",
       "      <td>cyclotomis</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>cyclotactic wave</td>\n",
       "      <td>cyclotactic event</td>\n",
       "      <td>cyclosis</td>\n",
       "      <td>tidal wave</td>\n",
       "      <td>taijiquan</td>\n",
       "      <td>Typhoon Haiyan</td>\n",
       "      <td>cyclotomavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Q102513]</td>\n",
       "      <td>Who wrote the novel \"uncle Tom's Cabin\"?</td>\n",
       "      <td>Harriet Beecher Stowe</td>\n",
       "      <td>Harriet Beecher Stowe</td>\n",
       "      <td>Harriet Beecher Stowe</td>\n",
       "      <td>Harriet Beecher Stowe</td>\n",
       "      <td>Harlan Ellison</td>\n",
       "      <td>Harriet Beecher Stowe</td>\n",
       "      <td>Harlan Ellison</td>\n",
       "      <td>Harriet Beecher Stowe</td>\n",
       "      <td>...</td>\n",
       "      <td>Anne McCaffrey</td>\n",
       "      <td>Harlan Ellison</td>\n",
       "      <td>Harriet Beecher Stowe.</td>\n",
       "      <td>Lynda Benson</td>\n",
       "      <td>Ernest Hemingway</td>\n",
       "      <td>Louisa May Alcott</td>\n",
       "      <td>Lyndall McCord</td>\n",
       "      <td>Haruki Murakami</td>\n",
       "      <td>Harriet Monroe Cooke</td>\n",
       "      <td>Herman Melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Q692]</td>\n",
       "      <td>Who is the author of the play \"Romeo and Juliet\"?</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>...</td>\n",
       "      <td>Juliet of the Alps</td>\n",
       "      <td>Jean Racine</td>\n",
       "      <td>Richard Brinsley Shaw</td>\n",
       "      <td>Henry Fielding</td>\n",
       "      <td>Ben Travers</td>\n",
       "      <td>A. R. Toklas</td>\n",
       "      <td>Jean Genet</td>\n",
       "      <td>Juliet Prowse</td>\n",
       "      <td>Henry Fielding, Jr.</td>\n",
       "      <td>Richard Brinsley Hayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Q19660]</td>\n",
       "      <td>What is the name of the capital of Romania?</td>\n",
       "      <td>Bucharest</td>\n",
       "      <td>Bucharest</td>\n",
       "      <td>Bucharest</td>\n",
       "      <td>Bucharest</td>\n",
       "      <td>Bucharest</td>\n",
       "      <td>Bucharest</td>\n",
       "      <td>Bucharest</td>\n",
       "      <td>Bucharest</td>\n",
       "      <td>...</td>\n",
       "      <td>Ploiești County</td>\n",
       "      <td>Bucharest International Airport</td>\n",
       "      <td>Chişinău</td>\n",
       "      <td>Bucharest City Hall</td>\n",
       "      <td>Chișinău</td>\n",
       "      <td>Cluj-Napoca</td>\n",
       "      <td>Bucharest Capital City</td>\n",
       "      <td>Dobrudja</td>\n",
       "      <td>Bucharest Capital</td>\n",
       "      <td>Ploieşti County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Q6607, Q17172850, Q483994, Q626035, Q2643890]</td>\n",
       "      <td>What instrument did Jimi Hendrix play?</td>\n",
       "      <td>guitar</td>\n",
       "      <td>guitar</td>\n",
       "      <td>guitar</td>\n",
       "      <td>guitar</td>\n",
       "      <td>guitar</td>\n",
       "      <td>guitar</td>\n",
       "      <td>guitar</td>\n",
       "      <td>guitar</td>\n",
       "      <td>...</td>\n",
       "      <td>elm guitar</td>\n",
       "      <td>Fender Rhodes</td>\n",
       "      <td>MIDI</td>\n",
       "      <td>Fender Telecaster Deluxe</td>\n",
       "      <td>Fender Bass Guitar</td>\n",
       "      <td>banjo</td>\n",
       "      <td>cello</td>\n",
       "      <td>harpy guitar</td>\n",
       "      <td>ukulele</td>\n",
       "      <td>clarinet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   O  \\\n",
       "0  [Q7944, Q167903, Q5975740, Q60186, Q2580904, Q...   \n",
       "1                                          [Q102513]   \n",
       "2                                             [Q692]   \n",
       "3                                           [Q19660]   \n",
       "4     [Q6607, Q17172850, Q483994, Q626035, Q2643890]   \n",
       "\n",
       "                                                   Q               answer_0  \\\n",
       "0                          What can cause a tsunami?                tsunami   \n",
       "1           Who wrote the novel \"uncle Tom's Cabin\"?  Harriet Beecher Stowe   \n",
       "2  Who is the author of the play \"Romeo and Juliet\"?    William Shakespeare   \n",
       "3        What is the name of the capital of Romania?              Bucharest   \n",
       "4             What instrument did Jimi Hendrix play?                 guitar   \n",
       "\n",
       "                answer_1               answer_2               answer_3  \\\n",
       "0                tsunami                tsunami                tsunami   \n",
       "1  Harriet Beecher Stowe  Harriet Beecher Stowe  Harriet Beecher Stowe   \n",
       "2    William Shakespeare    William Shakespeare    William Shakespeare   \n",
       "3              Bucharest              Bucharest              Bucharest   \n",
       "4                 guitar                 guitar                 guitar   \n",
       "\n",
       "              answer_4               answer_5             answer_6  \\\n",
       "0              tsunami                tsunami              tsunami   \n",
       "1       Harlan Ellison  Harriet Beecher Stowe       Harlan Ellison   \n",
       "2  William Shakespeare    William Shakespeare  William Shakespeare   \n",
       "3            Bucharest              Bucharest            Bucharest   \n",
       "4               guitar                 guitar               guitar   \n",
       "\n",
       "                answer_7  ...          answer_190  \\\n",
       "0                tsunami  ...        actinopteryx   \n",
       "1  Harriet Beecher Stowe  ...      Anne McCaffrey   \n",
       "2    William Shakespeare  ...  Juliet of the Alps   \n",
       "3              Bucharest  ...     Ploiești County   \n",
       "4                 guitar  ...          elm guitar   \n",
       "\n",
       "                        answer_191              answer_192  \\\n",
       "0                       cyclotomis                 tsunami   \n",
       "1                   Harlan Ellison  Harriet Beecher Stowe.   \n",
       "2                      Jean Racine   Richard Brinsley Shaw   \n",
       "3  Bucharest International Airport                Chişinău   \n",
       "4                    Fender Rhodes                    MIDI   \n",
       "\n",
       "                 answer_193          answer_194         answer_195  \\\n",
       "0          cyclotactic wave   cyclotactic event           cyclosis   \n",
       "1              Lynda Benson    Ernest Hemingway  Louisa May Alcott   \n",
       "2            Henry Fielding         Ben Travers       A. R. Toklas   \n",
       "3       Bucharest City Hall            Chișinău        Cluj-Napoca   \n",
       "4  Fender Telecaster Deluxe  Fender Bass Guitar              banjo   \n",
       "\n",
       "               answer_196       answer_197            answer_198  \\\n",
       "0              tidal wave        taijiquan        Typhoon Haiyan   \n",
       "1          Lyndall McCord  Haruki Murakami  Harriet Monroe Cooke   \n",
       "2              Jean Genet    Juliet Prowse   Henry Fielding, Jr.   \n",
       "3  Bucharest Capital City         Dobrudja     Bucharest Capital   \n",
       "4                   cello     harpy guitar               ukulele   \n",
       "\n",
       "               answer_199  \n",
       "0          cyclotomavirus  \n",
       "1         Herman Melville  \n",
       "2  Richard Brinsley Hayes  \n",
       "3         Ploieşti County  \n",
       "4                clarinet  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df = pd.DataFrame(\n",
    "    {\n",
    "        \"O\": test_dataset[\"object\"],\n",
    "        \"Q\": test_dataset[\"question\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "test_with_answers_df = pd.concat([_df, pd.DataFrame(generated_decoded)], axis=1)\n",
    "test_with_answers_df.to_csv(\"rubq_test_with_answers_no_prefix.csv\", index=False)\n",
    "test_with_answers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    get_best_checkpoint_path(\n",
    "        Path(\"../../../runs/wdsq_tunned/google_t5-large-ssm/models/\")\n",
    "    )\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "num_return_sequences = 200\n",
    "num_beams = 200\n",
    "num_beam_groups = 20\n",
    "diversity_penalty = 0.1\n",
    "batch_size = 2\n",
    "\n",
    "dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "generated_decoded = {f\"answer_{idx}\": [] for idx in range(num_return_sequences)}\n",
    "for batch in tqdm(dataloader, desc=\"evaluate model\"):\n",
    "    generated_ids = model.generate(\n",
    "        batch[\"input_ids\"].to(device),\n",
    "        num_beams=num_beams,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        num_beam_groups=num_beam_groups,\n",
    "        diversity_penalty=diversity_penalty,\n",
    "        # prefix_allowed_tokens_fn=prefix_allowed_tokens,\n",
    "    )\n",
    "    generated_decoded_batch = tokenizer.batch_decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "    )\n",
    "\n",
    "    current_batch_size = batch[\"input_ids\"].shape[0]\n",
    "    for start_batch_idx in range(\n",
    "        0, current_batch_size * num_return_sequences, num_return_sequences\n",
    "    ):\n",
    "        for answer_idx, answer in enumerate(\n",
    "            generated_decoded_batch[\n",
    "                start_batch_idx : start_batch_idx + num_return_sequences\n",
    "            ]\n",
    "        ):\n",
    "            generated_decoded[f\"answer_{answer_idx}\"].append(answer)\n",
    "\n",
    "test_with_answers_no_prefix_df = pd.concat(\n",
    "    [test_df, pd.DataFrame(generated_decoded)], axis=1\n",
    ")\n",
    "test_with_answers_no_prefix_df.to_csv(\"test_with_answers_no_prefix.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "questions = np.load(\"./questions_test.npy\")\n",
    "answers = np.load(\"./answers_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Что может вызвать цунами?',\n",
       "       'Кто написал роман «Хижина дяди Тома»?',\n",
       "       'Кто автор пьесы «Ромео и Джульетта»?', ...,\n",
       "       'Кто нарисовал черный квадрат?',\n",
       "       'Кто разработал клеточную теорию?',\n",
       "       'В каких единицах измеряется масса?'], dtype='<U144')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Q7944', 'Q60186', 'Q167903', 'Q2580904', 'Q5975740', 'Q7692360']),\n",
       "       list(['Q102513']), list(['Q692']), ..., list(['Q130777']),\n",
       "       list(['Q76432', 'Q76745', 'Q76747']), list(['Q11570'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
