{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from typing import List, Optional\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import sys\n",
<<<<<<< HEAD
    "\n",
    "sys.path.insert(0, \"..\")\n",
=======
    "sys.path.insert(0,'..')\n",
>>>>>>> f05ffd290a842160b134ded9b19c9816801178c0
    "\n",
    "from config import SPARQL_ENDPOINT\n",
    "from wikidata.wikidata_entity_to_label import WikidataEntityToLabel\n",
    "\n",
    "from seq2seq.utils import load_model_and_tokenizer_by_name, convert_to_features\n",
    "from seq2seq.train import train as train_seq2seq\n",
    "from seq2seq.eval import make_report\n",
    "from wikidata.wikidata_redirects import WikidataRedirectsCache\n",
    "from metrics import recall\n",
    "from utils.train_eval import get_best_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'wikidata-simplequestions' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/askplatypus/wikidata-simplequestions.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_number_of_answers(Q, P, direction):\n",
    "    query_string = \"\"\"\n",
    "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "        SELECT (COUNT(*) as ?count) WHERE { wd:%s %swdt:%s ?item.}\n",
<<<<<<< HEAD
    "        \"\"\"\n",
    "    if direction == \"inverse\":\n",
    "        d = \"^\"\n",
    "    else:\n",
    "        d = \"\"\n",
    "    query = query_string % (Q, d, P)\n",
    "    response = requests.get(\n",
    "        SPARQL_ENDPOINT,\n",
    "        params={\"format\": \"json\", \"query\": query},\n",
=======
    "        \"\"\" \n",
    "    if direction == 'inverse':\n",
    "        d = '^'\n",
    "    else:\n",
    "        d = ''\n",
    "    query = query_string  % (Q, d, P)\n",
    "    response = requests.get(\n",
    "        SPARQL_ENDPOINT,\n",
    "        params={'format': 'json', 'query': query},\n",
>>>>>>> f05ffd290a842160b134ded9b19c9816801178c0
    "        headers={\"Accept\": \"application/json\"},\n",
    "    )\n",
    "    to_sleep = 0.2\n",
    "    while response.status_code == 429:\n",
<<<<<<< HEAD
    "        if \"retry-after\" in response.headers:\n",
    "            to_sleep += int(response.headers[\"retry-after\"])\n",
    "        to_sleep += 0.5\n",
    "        time.sleep(to_sleep)\n",
    "        response = requests.get(\n",
    "            SPARQL_ENDPOINT,\n",
    "            params={\"format\": \"json\", \"query\": query},\n",
    "            headers={\"Accept\": \"application/json\"},\n",
    "        )\n",
    "    return response.json()[\"results\"][\"bindings\"][0][\"count\"][\"value\"]\n",
    "\n",
    "\n",
    "def get_number_of_answers(row):\n",
    "    predicate = row[\"P\"]\n",
    "    if predicate[0] == \"R\":\n",
    "        direction = \"inverse\"\n",
    "        predicate = predicate.replace(\"R\", \"P\")\n",
    "    else:\n",
    "        direction = \"direct\"\n",
    "\n",
    "    return request_number_of_answers(row[\"S\"], predicate, direction)"
=======
    "        if 'retry-after' in response.headers:\n",
    "            to_sleep += int(response.headers['retry-after'])\n",
    "        to_sleep += 0.5 \n",
    "        time.sleep(to_sleep)\n",
    "        response = requests.get(\n",
    "            SPARQL_ENDPOINT,\n",
    "            params={'format': 'json', 'query': query},\n",
    "            headers={\"Accept\": \"application/json\"},\n",
    "        )\n",
    "    return response.json()[\"results\"][\"bindings\"][0]['count']['value']\n",
    "\n",
    "\n",
    "def get_number_of_answers(row):\n",
    "    predicate = row['P']\n",
    "    if predicate[0] == 'R':\n",
    "        direction = 'inverse'\n",
    "        predicate = predicate.replace('R', 'P')\n",
    "    else:\n",
    "        direction = 'direct'\n",
    "\n",
    "    return request_number_of_answers(row['S'], predicate, direction) \n"
>>>>>>> f05ffd290a842160b134ded9b19c9816801178c0
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 46.33it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 71.06it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 53.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>P</th>\n",
       "      <th>O</th>\n",
       "      <th>Q</th>\n",
       "      <th>number_of_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q7358590</td>\n",
       "      <td>P20</td>\n",
       "      <td>Q1637790</td>\n",
       "      <td>Where did roger marquis die</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q154335</td>\n",
       "      <td>P509</td>\n",
       "      <td>Q12152</td>\n",
       "      <td>what was the cause of death of yves klein</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q2747238</td>\n",
       "      <td>P413</td>\n",
       "      <td>Q5059480</td>\n",
       "      <td>What position does carlos gomez play?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q62498</td>\n",
       "      <td>P21</td>\n",
       "      <td>Q6581097</td>\n",
       "      <td>how does engelbert zaschka identify</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q182485</td>\n",
       "      <td>P413</td>\n",
       "      <td>Q1143358</td>\n",
       "      <td>what position does pee wee reese play in baseball</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          S     P         O  \\\n",
       "0  Q7358590   P20  Q1637790   \n",
       "1   Q154335  P509    Q12152   \n",
       "2  Q2747238  P413  Q5059480   \n",
       "3    Q62498   P21  Q6581097   \n",
       "4   Q182485  P413  Q1143358   \n",
       "\n",
       "                                                   Q number_of_answers  \n",
       "0                        Where did roger marquis die                 0  \n",
       "1          what was the cause of death of yves klein                 1  \n",
       "2              What position does carlos gomez play?                 1  \n",
       "3               how does engelbert zaschka identify                  1  \n",
       "4  what position does pee wee reese play in baseball                 0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\n",
<<<<<<< HEAD
    "    \"./wikidata-simplequestions/annotated_wd_data_train_answerable.txt\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"S\", \"P\", \"O\", \"Q\"],\n",
    "    nrows=100,\n",
    ")\n",
    "train_df[\"number_of_answers\"] = train_df.progress_apply(get_number_of_answers, axis=1)\n",
    "\n",
    "valid_df = pd.read_csv(\n",
    "    \"./wikidata-simplequestions/annotated_wd_data_valid_answerable.txt\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"S\", \"P\", \"O\", \"Q\"],\n",
    "    nrows=100,\n",
    ")\n",
    "valid_df[\"number_of_answers\"] = valid_df.progress_apply(get_number_of_answers, axis=1)\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    \"./wikidata-simplequestions/annotated_wd_data_test_answerable.txt\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"S\", \"P\", \"O\", \"Q\"],\n",
    "    # nrows=200,\n",
    ")\n",
    "test_df[\"number_of_answers\"] = test_df.progress_apply(get_number_of_answers, axis=1)\n",
=======
    "    './wikidata-simplequestions/annotated_wd_data_train_answerable.txt',\n",
    "    sep='\\t',\n",
    "    names=['S', 'P', 'O', 'Q'],\n",
    "    nrows=100,\n",
    ")\n",
    "train_df['number_of_answers'] = train_df.progress_apply(get_number_of_answers, axis=1)\n",
    "\n",
    "valid_df = pd.read_csv(\n",
    "    './wikidata-simplequestions/annotated_wd_data_valid_answerable.txt',\n",
    "    sep='\\t',\n",
    "    names=['S', 'P', 'O', 'Q'],\n",
    "    nrows=100,\n",
    ")\n",
    "valid_df['number_of_answers'] = valid_df.progress_apply(get_number_of_answers, axis=1)\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    './wikidata-simplequestions/annotated_wd_data_test_answerable.txt',\n",
    "    sep='\\t',\n",
    "    names=['S', 'P', 'O', 'Q'],\n",
    "    # nrows=200,\n",
    ")\n",
    "test_df['number_of_answers'] = test_df.progress_apply(get_number_of_answers, axis=1)\n",
>>>>>>> f05ffd290a842160b134ded9b19c9816801178c0
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 172.79it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 130.84it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 154.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of full train:  100\n",
      "Size of full valid:  100\n",
      "Size of cleared train:  50\n",
      "Size of cleared valid:  55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wikidata_entity_to_label = WikidataEntityToLabel()\n",
    "\n",
<<<<<<< HEAD
    "train_df[\"object\"] = train_df[\"O\"].progress_apply(wikidata_entity_to_label.get_label)\n",
    "valid_df[\"object\"] = valid_df[\"O\"].progress_apply(wikidata_entity_to_label.get_label)\n",
    "test_df[\"object\"] = test_df[\"O\"].progress_apply(wikidata_entity_to_label.get_label)\n",
    "\n",
    "train_df = train_df.rename(columns={\"Q\": \"question\"})\n",
    "valid_df = valid_df.rename(columns={\"Q\": \"question\"})\n",
    "test_df = test_df.rename(columns={\"Q\": \"question\"})\n",
    "\n",
    "train_df[\"question\"] = train_df[\"question\"].astype(str)\n",
    "valid_df[\"question\"] = valid_df[\"question\"].astype(str)\n",
    "test_df[\"question\"] = test_df[\"question\"].astype(str)\n",
    "\n",
    "train_df[\"object\"] = train_df[\"object\"].astype(str)\n",
    "valid_df[\"object\"] = valid_df[\"object\"].astype(str)\n",
    "test_df[\"object\"] = test_df[\"object\"].astype(str)\n",
    "\n",
    "print(\"Size of full train: \", train_df.index.size)\n",
    "print(\"Size of full valid: \", valid_df.index.size)\n",
    "\n",
    "train_df = train_df[train_df[\"number_of_answers\"].astype(int) == 1]\n",
    "valid_df = valid_df[valid_df[\"number_of_answers\"].astype(int) == 1]\n",
    "\n",
    "print(\"Size of cleared train: \", train_df.index.size)\n",
    "print(\"Size of cleared valid: \", valid_df.index.size)\n",
    "\n",
    "\n",
    "train_dataset = datasets.Dataset.from_pandas(train_df)\n",
    "valid_dataset = datasets.Dataset.from_pandas(valid_df)"
=======
    "train_df['object'] = train_df['O'].progress_apply(\n",
    "    wikidata_entity_to_label.get_label\n",
    ")\n",
    "valid_df['object'] = valid_df['O'].progress_apply(\n",
    "    wikidata_entity_to_label.get_label\n",
    ")\n",
    "test_df['object'] = test_df['O'].progress_apply(\n",
    "    wikidata_entity_to_label.get_label\n",
    ")\n",
    "\n",
    "train_df = train_df.rename(columns={'Q': 'question'})\n",
    "valid_df = valid_df.rename(columns={'Q': 'question'})\n",
    "test_df = test_df.rename(columns={'Q': 'question'})\n",
    "\n",
    "train_df['question'] = train_df['question'].astype(str)\n",
    "valid_df['question'] = valid_df['question'].astype(str)\n",
    "test_df['question'] = test_df['question'].astype(str)\n",
    "\n",
    "train_df['object'] = train_df['object'].astype(str)\n",
    "valid_df['object'] = valid_df['object'].astype(str)\n",
    "test_df['object'] = test_df['object'].astype(str)\n",
    "\n",
    "print('Size of full train: ', train_df.index.size)\n",
    "print('Size of full valid: ', valid_df.index.size)\n",
    "\n",
    "train_df = train_df[train_df['number_of_answers'].astype(int)  == 1]\n",
    "valid_df = valid_df[valid_df['number_of_answers'].astype(int)  == 1]\n",
    "\n",
    "print('Size of cleared train: ', train_df.index.size)\n",
    "print('Size of cleared valid: ', valid_df.index.size)\n",
    "\n",
    "\n",
    "train_dataset = datasets.Dataset.from_pandas(train_df)\n",
    "valid_dataset = datasets.Dataset.from_pandas(valid_df)\n"
>>>>>>> f05ffd290a842160b134ded9b19c9816801178c0
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "t5-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.11ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 49.33ba/s]\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "model_name = \"t5-large\"\n",
    "print(f\"\\n\\n{model_name}\")\n",
=======
    "model_name = 't5-large'\n",
    "print(f'\\n\\n{model_name}')\n",
>>>>>>> f05ffd290a842160b134ded9b19c9816801178c0
    "model, tokenizer = load_model_and_tokenizer_by_name(model_name)\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda batch: convert_to_features(batch, tokenizer),\n",
    "    batched=True,\n",
    ")\n",
    "valid_dataset = valid_dataset.map(\n",
    "    lambda batch: convert_to_features(batch, tokenizer),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "columns = [\n",
    "    \"input_ids\",\n",
    "    \"labels\",\n",
    "    \"attention_mask\",\n",
    "]\n",
    "train_dataset.set_format(type=\"torch\", columns=columns)\n",
    "valid_dataset.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "model_path = get_best_checkpoint_path(\n",
    "    f\"../../runs/cleared_wikidata/without_redirects_augmentation/{model_name}/\"\n",
    ")\n",
    "model, tokenizer = load_model_and_tokenizer_by_name(model_name, model_path)\n",
    "model = model.to(\"cuda\")"
=======
    "model_path = get_best_checkpoint_path(f'../../runs/cleared_wikidata/without_redirects_augmentation/{model_name}/')\n",
    "model, tokenizer = load_model_and_tokenizer_by_name(\n",
    "    model_name, model_path\n",
    ")\n",
    "model = model.to('cuda')"
>>>>>>> f05ffd290a842160b134ded9b19c9816801178c0
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = train_seq2seq(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     train_dataset=train_dataset,\n",
    "#     valid_dataset=valid_dataset,\n",
    "#     output_dir=f'../../runs/cleared_wikidata/without_redirects_augmentation/{model_name}/',\n",
    "#     logging_dir=f'../../runs/cleared_wikidata/without_redirects_augmentation/{model_name}/',\n",
    "#     num_train_epochs=8,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     eval_steps=500,\n",
    "#     logging_steps=500,\n",
    "#     trainer_mode='default',\n",
    "# )\n",
    "\n",
    "# del trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beams = 100\n",
    "num_beam_groups = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.43ba/s]\n",
      "evaluate model:   0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:197: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  warnings.warn(\n",
      "evaluate model:   0%|          | 1/200 [00:29<1:37:37, 29.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4 .ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m test_dataset \u001b[39m=\u001b[39m test_dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m batch: convert_to_features(batch, tokenizer),\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m test_dataset\u001b[39m.\u001b[39mset_format(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m, columns\u001b[39m=\u001b[39mcolumns)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m results_df_full, report_full \u001b[39m=\u001b[39m make_report(\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     dataset\u001b[39m=\u001b[39;49mtest_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     num_beams\u001b[39m=\u001b[39;49mnum_beams,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49mnum_beams,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     num_beam_groups\u001b[39m=\u001b[39;49mnum_beam_groups,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     diversity_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mif\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_available() \u001b[39melse\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m results_df_full\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../../runs/cleared_wikidata/without_redirects_augmentation/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m/results_full.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73616c6e696b6f765f67707533222c2273657474696e6773223a7b22636f6e74657874223a226e6c7031227d7d/workspace/kbqa/experiments/cleared_simplequestions_seq2seq_i4%20.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m answer_cols \u001b[39m=\u001b[39m [col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m results_df_full\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39manswer_\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m col]\n",
      "File \u001b[0;32m/workspace/kbqa/experiments/../seq2seq/eval.py:137\u001b[0m, in \u001b[0;36mmake_report\u001b[0;34m(model, tokenizer, dataset, batch_size, num_beams, num_return_sequences, num_beam_groups, diversity_penalty, device)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39m\"\"\"make_report\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39m    Tuple[pd.DataFrame, dict]: results of predicting with questions on DataFrame and report dict with metrics\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m results_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[1;32m    135\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m: dataset[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m: dataset[\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m    136\u001b[0m )\n\u001b[0;32m--> 137\u001b[0m generated_answers \u001b[39m=\u001b[39m predict_answers(\n\u001b[1;32m    138\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    139\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m    140\u001b[0m     dataset\u001b[39m=\u001b[39;49mdataset,\n\u001b[1;32m    141\u001b[0m     num_beams\u001b[39m=\u001b[39;49mnum_beams,\n\u001b[1;32m    142\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49mnum_return_sequences,\n\u001b[1;32m    143\u001b[0m     num_beam_groups\u001b[39m=\u001b[39;49mnum_beam_groups,\n\u001b[1;32m    144\u001b[0m     diversity_penalty\u001b[39m=\u001b[39;49mdiversity_penalty,\n\u001b[1;32m    145\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    146\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    147\u001b[0m )\n\u001b[1;32m    148\u001b[0m \u001b[39mfor\u001b[39;00m key, vals \u001b[39min\u001b[39;00m generated_answers\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    149\u001b[0m     results_df[key] \u001b[39m=\u001b[39m vals\n",
      "File \u001b[0;32m/workspace/kbqa/experiments/../seq2seq/eval.py:42\u001b[0m, in \u001b[0;36mpredict_answers\u001b[0;34m(model, tokenizer, dataset, batch_size, num_beams, num_return_sequences, num_beam_groups, diversity_penalty, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m generated_decoded \u001b[39m=\u001b[39m {\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39manswer_\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m: [] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_return_sequences)}\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(dataloader, \u001b[39m\"\u001b[39m\u001b[39mevaluate model\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 42\u001b[0m     generated_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     43\u001b[0m         batch[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m     44\u001b[0m         num_beams\u001b[39m=\u001b[39;49mnum_beams,\n\u001b[1;32m     45\u001b[0m         num_return_sequences\u001b[39m=\u001b[39;49mnum_return_sequences,\n\u001b[1;32m     46\u001b[0m         num_beam_groups\u001b[39m=\u001b[39;49mnum_beam_groups,\n\u001b[1;32m     47\u001b[0m         diversity_penalty\u001b[39m=\u001b[39;49mdiversity_penalty,\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     generated_decoded_batch \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(\n\u001b[1;32m     50\u001b[0m         generated_ids,\n\u001b[1;32m     51\u001b[0m         skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m         clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     55\u001b[0m     current_batch_size \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1444\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1441\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[1;32m   1443\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[0;32m-> 1444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup_beam_search(\n\u001b[1;32m   1445\u001b[0m         input_ids,\n\u001b[1;32m   1446\u001b[0m         beam_scorer,\n\u001b[1;32m   1447\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1448\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1449\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   1450\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   1451\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   1452\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1453\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1454\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1455\u001b[0m     )\n\u001b[1;32m   1457\u001b[0m \u001b[39melif\u001b[39;00m is_constraint_gen_mode:\n\u001b[1;32m   1458\u001b[0m     \u001b[39mif\u001b[39;00m num_return_sequences \u001b[39m>\u001b[39m num_beams:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:2854\u001b[0m, in \u001b[0;36mGenerationMixin.group_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2852\u001b[0m \u001b[39m# do one decoder step on all beams of all sentences in batch\u001b[39;00m\n\u001b[1;32m   2853\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2854\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2855\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2856\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2857\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2858\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2859\u001b[0m )\n\u001b[1;32m   2861\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2862\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py:1639\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1636\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1638\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> 1639\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1640\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1641\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1642\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1643\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1644\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   1645\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1646\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1647\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1648\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1649\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1650\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1651\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1652\u001b[0m )\n\u001b[1;32m   1654\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1656\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py:1035\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m   1023\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1024\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     )\n\u001b[1;32m   1034\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1035\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1036\u001b[0m         hidden_states,\n\u001b[1;32m   1037\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1038\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   1039\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1040\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1041\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   1042\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1043\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1044\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1045\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1046\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1047\u001b[0m     )\n\u001b[1;32m   1049\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py:718\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    715\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    717\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n\u001b[1;32m    720\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16 \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39misinf(hidden_states)\u001b[39m.\u001b[39many():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py:328\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    327\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 328\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDenseReluDense(forwarded_states)\n\u001b[1;32m    329\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    330\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py:293\u001b[0m, in \u001b[0;36mT5DenseActDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    291\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(hidden_states)\n\u001b[1;32m    292\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> 293\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwo(hidden_states)\n\u001b[1;32m    294\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
=======
    "\n",
>>>>>>> f05ffd290a842160b134ded9b19c9816801178c0
    "test_dataset = datasets.Dataset.from_pandas(test_df)\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda batch: convert_to_features(batch, tokenizer),\n",
    "    batched=True,\n",
    ")\n",
    "test_dataset.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "\n",
    "results_df_full, report_full = make_report(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=test_dataset,\n",
    "    batch_size=1,\n",
    "    num_beams=num_beams,\n",
    "    num_return_sequences=num_beams,\n",
    "    num_beam_groups=num_beam_groups,\n",
    "    diversity_penalty=0.2,\n",
<<<<<<< HEAD
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")\n",
    "\n",
    "results_df_full.to_csv(\n",
    "    f\"../../runs/cleared_wikidata/without_redirects_augmentation/{model_name}/results_full.csv\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "\n",
    "answer_cols = [col for col in results_df_full.columns if \"answer_\" in col]\n",
    "wrc = WikidataRedirectsCache()\n",
    "\n",
    "recall_full = recall(\n",
    "    results_df_full[\"target\"].values,\n",
    "    results_df_full[answer_cols].values,\n",
    ")\n",
    "recall_full_with_redirects = recall(\n",
    "    results_df_full[\"target\"].values, results_df_full[answer_cols].values, wrc\n",
    ")\n",
    "\n",
    "print(\"\\n\\nFULL\")\n",
=======
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    ")\n",
    "\n",
    "results_df_full.to_csv(f'../../runs/cleared_wikidata/without_redirects_augmentation/{model_name}/results_full.csv', index=False)\n",
    "\n",
    "\n",
    "answer_cols = [col for col in results_df_full.columns if 'answer_' in col]\n",
    "wrc = WikidataRedirectsCache()\n",
    "\n",
    "recall_full = recall(\n",
    "    results_df_full['target'].values,\n",
    "    results_df_full[answer_cols].values,\n",
    ")\n",
    "recall_full_with_redirects = recall(\n",
    "    results_df_full['target'].values,\n",
    "    results_df_full[answer_cols].values,\n",
    "    wrc\n",
    ")\n",
    "\n",
    "print('\\n\\nFULL')\n",
>>>>>>> f05ffd290a842160b134ded9b19c9816801178c0
    "pprint(report_full)\n",
    "print(f\"Recall: {recall_full}\")\n",
    "print(f\"Recall with respect to redirects: {recall_full_with_redirects}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For test set with only 1 number of answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "test_dataset = datasets.Dataset.from_pandas(\n",
    "    test_df[test_df[\"number_of_answers\"].astype(int) == 1]\n",
    ")\n",
=======
    "test_dataset = datasets.Dataset.from_pandas(test_df[test_df['number_of_answers'].astype(int) == 1])\n",
>>>>>>> f05ffd290a842160b134ded9b19c9816801178c0
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda batch: convert_to_features(batch, tokenizer),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "test_dataset.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "results_df_only1, report_only1 = make_report(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=test_dataset,\n",
    "    batch_size=1,\n",
    "    num_beams=num_beams,\n",
    "    num_return_sequences=num_beams,\n",
    "    num_beam_groups=num_beam_groups,\n",
    "    diversity_penalty=0.2,\n",
<<<<<<< HEAD
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")\n",
    "\n",
    "results_df_only1.to_csv(\n",
    "    f\"../../runs/cleared_wikidata/without_redirects_augmentation/{model_name}/results_only1_answer.csv\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "recall_only1 = recall(\n",
    "    results_df_only1[\"target\"].values,\n",
    "    results_df_only1[answer_cols].values,\n",
    ")\n",
    "recall_only1_with_redirects = recall(\n",
    "    results_df_only1[\"target\"].values, results_df_only1[answer_cols].values, wrc\n",
    ")\n",
    "\n",
    "print(\"\\n\\nONLY 1\")\n",
    "pprint(report_only1)\n",
    "print(f\"Recall: {recall_only1}\")\n",
    "print(f\"Recall with respect to redirects: {recall_only1_with_redirects}\")"
=======
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    ")\n",
    "\n",
    "results_df_only1.to_csv(f'../../runs/cleared_wikidata/without_redirects_augmentation/{model_name}/results_only1_answer.csv', index=False)\n",
    "\n",
    "recall_only1 = recall(\n",
    "    results_df_only1['target'].values,\n",
    "    results_df_only1[answer_cols].values,\n",
    ")\n",
    "recall_only1_with_redirects = recall(\n",
    "    results_df_only1['target'].values,\n",
    "    results_df_only1[answer_cols].values,\n",
    "    wrc\n",
    ")\n",
    "\n",
    "print('\\n\\nONLY 1')\n",
    "pprint(report_only1)\n",
    "print(f\"Recall: {recall_only1}\")\n",
    "print(f\"Recall with respect to redirects: {recall_only1_with_redirects}\")\n"
>>>>>>> f05ffd290a842160b134ded9b19c9816801178c0
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
