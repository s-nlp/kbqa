{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset \n",
    "\n",
    "(1-hop subset from question's entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: load the prefix tree (trie), you need to additionally download\n",
    "# https://huggingface.co/facebook/mgenre-wiki/blob/main/trie.py and \n",
    "# https://huggingface.co/facebook/mgenre-wiki/blob/main/titles_lang_all105_trie_with_redirect.pkl\n",
    "# that is fast but memory inefficient prefix tree (trie) -- it is implemented with nested python `dict`\n",
    "# NOTE: loading this map may take up to 10 minutes and occupy a lot of RAM!\n",
    "# import pickle\n",
    "# from trie import Trie\n",
    "# with open(\"titles_lang_all105_marisa_trie_with_redirect.pkl\", \"rb\") as f:\n",
    "#     trie = Trie.load_from_dict(pickle.load(f))\n",
    "\n",
    "# or a memory efficient but a bit slower prefix tree (trie) -- it is implemented with `marisa_trie` from\n",
    "# https://huggingface.co/facebook/mgenre-wiki/blob/main/titles_lang_all105_marisa_trie_with_redirect.pkl\n",
    "# from genre.trie import MarisaTrie\n",
    "# with open(\"titles_lang_all105_marisa_trie_with_redirect.pkl\", \"rb\") as f:\n",
    "#     trie = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘trie.py’ already there; not retrieving.\n",
      "\n",
      "File ‘titles_lang_all105_trie_with_redirect.pkl’ already there; not retrieving.\n",
      "\n",
      "File ‘lang_title2wikidataID-normalized_with_redirect.pkl’ already there; not retrieving.\n",
      "\n",
      "File ‘wikidataID2lang_title-normalized_with_redirect.pkl’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/facebookresearch/GENRE/main/genre/trie.py\n",
    "!wget -nc http://dl.fbaipublicfiles.com/GENRE/titles_lang_all105_trie_with_redirect.pkl\n",
    "!wget -nc https://dl.fbaipublicfiles.com/GENRE/lang_title2wikidataID-normalized_with_redirect.pkl\n",
    "!wget -nc https://dl.fbaipublicfiles.com/GENRE/wikidataID2lang_title-normalized_with_redirect.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from trie import Trie\n",
    "\n",
    "\n",
    "mgenre_tokenizer = AutoTokenizer.from_pretrained(\"facebook/mgenre-wiki\")\n",
    "mgenre_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mgenre-wiki\").eval()\n",
    "\n",
    "# with open(\"titles_lang_all105_trie_with_redirect.pkl\", \"rb\") as f:\n",
    "#     mgenre_trie = Trie.load_from_dict(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import Pipeline\n",
    "from functools import lru_cache\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from caches.ner_to_sentence_insertion import NerToSentenceInsertion\n",
    "\n",
    "import logging \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MGENREPipeline(Pipeline):\n",
    "    \"\"\"MGENREPipeline - HF Pipeline for mGENRE EntityLinking model\"\"\"\n",
    "\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        forward_kwargs = {}\n",
    "        if \"num_beams\" in kwargs:\n",
    "            forward_kwargs[\"num_beams\"] = kwargs.get(\"num_beams\", 10)\n",
    "        if \"num_return_sequences\" in kwargs:\n",
    "            forward_kwargs[\"num_return_sequences\"] = kwargs.get(\n",
    "                \"num_return_sequences\", 10\n",
    "            )\n",
    "        return {}, forward_kwargs, {}\n",
    "\n",
    "    def preprocess(self, input_):\n",
    "        return self.tokenizer(\n",
    "            input_,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    def _forward(\n",
    "        self,\n",
    "        input_tensors,\n",
    "        num_beams=10,\n",
    "        num_return_sequences=10,\n",
    "    ):\n",
    "        outputs = self.model.generate(\n",
    "            **{k: v.to(self.device) for k, v in input_tensors.items()},\n",
    "            num_beams=num_beams,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            # prefix_allowed_tokens_fn=lambda batch_id, sent: mgenre_trie.get(sent.tolist()),\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        outputs = self.tokenizer.batch_decode(\n",
    "            model_outputs,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class EntitiesSelection:\n",
    "    def __init__(self, ner_model):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.ner_model = ner_model\n",
    "\n",
    "    def entities_selection(self, entities_list, mgenre_predicted_entities_list):\n",
    "        final_preds = []\n",
    "\n",
    "        for pred_text in mgenre_predicted_entities_list:\n",
    "            labels = []\n",
    "            try:\n",
    "                _label, lang = pred_text.split(\" >> \")\n",
    "                if lang == \"en\":\n",
    "                    labels.append(_label)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error {str(e)} with pred_text={pred_text}\")\n",
    "\n",
    "            if len(labels) > 0:\n",
    "                for label in labels:\n",
    "                    label = label.lower()\n",
    "                    if self._check_label_fn(label, entities_list):\n",
    "                        final_preds.append(pred_text)\n",
    "\n",
    "        return list(dict.fromkeys(final_preds))\n",
    "\n",
    "    @lru_cache(maxsize=8192)\n",
    "    def _label_format_fn(self, label):\n",
    "        return \" \".join(\n",
    "            [self.stemmer.stem(str(token)) for token in self.ner_model(label)]\n",
    "        )\n",
    "\n",
    "    def _check_label_fn(self, label, entities_list):\n",
    "        label = self._label_format_fn(label)\n",
    "        for entity in entities_list:\n",
    "            entity = self._label_format_fn(entity)\n",
    "            if label == entity:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class LabelToEntity:\n",
    "    def __init__(\n",
    "        self,\n",
    "        title2wikidataID_path: str = 'lang_title2wikidataID-normalized_with_redirect.pkl',\n",
    "    ):\n",
    "        with open(title2wikidataID_path, \"rb\") as f:\n",
    "            self.lang_title_to_wikidata_id = pickle.load(f)\n",
    "\n",
    "    @lru_cache(maxsize=16384)\n",
    "    def __call__(self, text):\n",
    "        if ' >> ' not in text:\n",
    "            text += ' >> ln'\n",
    "\n",
    "        set_of_ids = self.lang_title_to_wikidata_id.get(\n",
    "            tuple(reversed(text.split(\" >> \")))\n",
    "        )\n",
    "        if set_of_ids is None:\n",
    "            return None\n",
    "        else:\n",
    "            return max(\n",
    "                set_of_ids,\n",
    "                key=lambda y: int(y[1:]),\n",
    "            )\n",
    "\n",
    "\n",
    "class EntityToLabel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        wikidataID2title_path: str = 'wikidataID2lang_title-normalized_with_redirect.pkl',\n",
    "        lang: str = 'en',\n",
    "    ):\n",
    "        with open(wikidataID2title_path, \"rb\") as f:\n",
    "            self.wikidata_id_lang_title = pickle.load(f)\n",
    "        \n",
    "        self.lang = lang\n",
    "\n",
    "    @lru_cache(maxsize=16384)\n",
    "    def __call__(self, index):\n",
    "        labels = self.wikidata_id_lang_title.get(index)\n",
    "        if labels is not None:\n",
    "            for _lng, label in labels:\n",
    "                if self.lang == _lng:\n",
    "                    return label\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "class EntityLinking():\n",
    "    def __init__(self, mgenre: MGENREPipeline, ner: NerToSentenceInsertion, label_to_entity: LabelToEntity):\n",
    "        self.mgenre = mgenre\n",
    "        self.ner = ner\n",
    "        self.entities_selection_module = EntitiesSelection(self.ner.model)\n",
    "        self.label_to_entity = label_to_entity\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        text_with_labeling, entities_list = self.ner.entity_labeling(text, True)\n",
    "        mgenre_predicted_entities_list = self.mgenre(text_with_labeling)\n",
    "        linked_entities_list = self.entities_selection_module.entities_selection(\n",
    "            entities_list, mgenre_predicted_entities_list\n",
    "        )\n",
    "        linked_entities_ids_list = [self.label_to_entity(label) for label in linked_entities_list]\n",
    "        return {\n",
    "            \"text_with_labeling\": text_with_labeling,\n",
    "            \"ner_entities\": entities_list,\n",
    "            \"mgenre_predicted_entities_list\": mgenre_predicted_entities_list,\n",
    "            \"linked_entities_list\": linked_entities_list,\n",
    "            \"linked_entities_ids_list\": linked_entities_ids_list,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = NerToSentenceInsertion(model_path='../../ner/model-best/')\n",
    "mgenre = MGENREPipeline(mgenre_model, mgenre_tokenizer)\n",
    "label_to_entity = LabelToEntity('./lang_title2wikidataID-normalized_with_redirect.pkl')\n",
    "entity_to_label = EntityToLabel('./wikidataID2lang_title-normalized_with_redirect.pkl')\n",
    "entity_linker = EntityLinking(\n",
    "    mgenre=mgenre,\n",
    "    ner=ner,\n",
    "    label_to_entity=label_to_entity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARQL_ENDPOINT = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "@lru_cache(maxsize=16384)\n",
    "def wikidata_request(query):\n",
    "    logger.info(f'Request to Wikidata with query:\\n{query}')\n",
    "    response = requests.get(\n",
    "        SPARQL_ENDPOINT,\n",
    "        params={\"format\": \"json\", \"query\": query},\n",
    "        headers={\"Accept\": \"application/json\"},\n",
    "    )\n",
    "    to_sleep = 0.2\n",
    "    while response.status_code == 429:\n",
    "        if \"retry-after\" in response.headers:\n",
    "            to_sleep += int(response.headers[\"retry-after\"])\n",
    "        to_sleep += 0.5\n",
    "        logger.info(f'wikidata_request to sleep...')\n",
    "        time.sleep(to_sleep)\n",
    "        response = requests.get(\n",
    "            SPARQL_ENDPOINT,\n",
    "            params={\"format\": \"json\", \"query\": query},\n",
    "            headers={\"Accept\": \"application/json\"},\n",
    "        )\n",
    "    return response.json()[\"results\"][\"bindings\"]\n",
    "\n",
    "def wikidata_get_corresponded_objects(entity):\n",
    "    query = \"\"\"\n",
    "SELECT ?p ?item WHERE {\n",
    "    wd:<E1> ?p ?item .\n",
    "    ?article schema:about ?item .\n",
    "    ?article schema:inLanguage \"en\" .\n",
    "    ?article schema:isPartOf <https://en.wikipedia.org/> .\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\n",
    "    \"\"\".replace('<E1>', entity)\n",
    "\n",
    "    cor_objects = wikidata_request(query)\n",
    "    cor_objects = [\n",
    "        val['item']['value'].split('/')[-1]\n",
    "        for val in cor_objects\n",
    "    ]\n",
    "    \n",
    "    return list(dict.fromkeys(cor_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "fatal: destination path 'wikidata-simplequestions' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/askplatypus/wikidata-simplequestions.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\n",
    "    './wikidata-simplequestions/annotated_wd_data_train_answerable.txt',\n",
    "    sep='\\t',\n",
    "    names=['S', 'P', 'O', 'Q'],\n",
    ")\n",
    "\n",
    "valid_df = pd.read_csv(\n",
    "    './wikidata-simplequestions/annotated_wd_data_valid_answerable.txt',\n",
    "    sep='\\t',\n",
    "    names=['S', 'P', 'O', 'Q'],\n",
    ")\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    './wikidata-simplequestions/annotated_wd_data_test_answerable.txt',\n",
    "    sep='\\t',\n",
    "    names=['S', 'P', 'O', 'Q'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=df.index.size):\n",
    "        linked_results = entity_linker(row['Q'])\n",
    "        linked_results['one_hop_neighbors'] = {\n",
    "            entity: [(eidx, entity_to_label(eidx)) for eidx in  wikidata_get_corresponded_objects(entity)]\n",
    "            for entity in linked_results['linked_entities_ids_list'] if entity is not None\n",
    "        }\n",
    "        results.append(\n",
    "            dict(**row.to_dict(), **linked_results)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0526f9f497240b0b212d38fbe6f316b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "new_valid_df = pd.DataFrame(prepare_dataset(valid_df.iloc[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
