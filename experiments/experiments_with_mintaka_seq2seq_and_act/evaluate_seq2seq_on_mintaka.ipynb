{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on task https://www.notion.so/msalnikov/b0b68b3db11b4c40a4bada127bfde310?v=635216a0f3d646d58fde31f60cc9e4c9&p=82caba2f68c94f4ea320134e855e7bb4&pm=c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "File ‘evaluate.py’ already there; not retrieving.\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2023-04-20 18:57:14--  https://github.com/amazon-science/mintaka/raw/main/data/mintaka_test.json\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/amazon-science/mintaka/main/data/mintaka_test.json [following]\n",
      "--2023-04-20 18:57:18--  https://raw.githubusercontent.com/amazon-science/mintaka/main/data/mintaka_test.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9607585 (9.2M) [text/plain]\n",
      "Saving to: ‘mintaka_test.json’\n",
      "\n",
      "mintaka_test.json   100%[===================>]   9.16M  24.8MB/s    in 0.4s    \n",
      "\n",
      "2023-04-20 18:57:20 (24.8 MB/s) - ‘mintaka_test.json’ saved [9607585/9607585]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/amazon-science/mintaka/main/evaluate/evaluate.py\n",
    "!wget -nc https://github.com/amazon-science/mintaka/raw/main/data/mintaka_test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import evaluate\n",
    "import requests\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from pywikidata import Entity\n",
    "from joblib import Memory\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from kbqa.seq2seq.utils import convert_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(8)\n",
    "random.seed(8)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory('/tmp/cache', verbose=0)\n",
    "\n",
    "@memory.cache\n",
    "def get_wd_search_results(\n",
    "    search_string: str,\n",
    "    max_results: int = 500,\n",
    "    language: str = 'en',\n",
    "    mediawiki_api_url: str = \"https://www.wikidata.org/w/api.php\",\n",
    "    user_agent: str = None,\n",
    ") -> list:\n",
    "    params = {\n",
    "        'action': 'wbsearchentities',\n",
    "        'language': language,\n",
    "        'search': search_string,\n",
    "        'format': 'json',\n",
    "        'limit': 50\n",
    "    }\n",
    "\n",
    "    user_agent = \"pywikidata\" if user_agent is None else user_agent\n",
    "    headers = {\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "\n",
    "    cont_count = 1\n",
    "    results = []\n",
    "    while cont_count > 0:\n",
    "        params.update({'continue': 0 if cont_count == 1 else cont_count})\n",
    "\n",
    "        reply = requests.get(mediawiki_api_url, params=params, headers=headers)\n",
    "        reply.raise_for_status()\n",
    "        search_results = reply.json()\n",
    "\n",
    "        if 'success' not in search_results or search_results['success'] != 1:\n",
    "            raise Exception('WD search failed')\n",
    "        else:\n",
    "            for i in search_results['search']:\n",
    "                results.append(i['id'])\n",
    "\n",
    "        if 'search-continue' not in search_results:\n",
    "            cont_count = 0\n",
    "        else:\n",
    "            cont_count = search_results['search-continue']\n",
    "\n",
    "        if cont_count > max_results:\n",
    "            break\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: MINTAKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: mintaka/en\n",
      "Found cached dataset mintaka (/root/.cache/huggingface/datasets/AmazonScience___mintaka/en/1.0.0/bb35d95f07aed78fa590601245009c5f585efe909dbd4a8f2a4025ccf65bb11d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ee042948b548769f8abbeafdc69c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'lang', 'question', 'answerText', 'category', 'complexityType', 'questionEntity', 'answerEntity'],\n",
       "        num_rows: 14000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'lang', 'question', 'answerText', 'category', 'complexityType', 'questionEntity', 'answerEntity'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'lang', 'question', 'answerText', 'category', 'complexityType', 'questionEntity', 'answerEntity'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('AmazonScience/mintaka')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>question</th>\n",
       "      <th>answerText</th>\n",
       "      <th>category</th>\n",
       "      <th>complexityType</th>\n",
       "      <th>questionEntity</th>\n",
       "      <th>answerEntity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9ace9041</td>\n",
       "      <td>en</td>\n",
       "      <td>What is the fourth book in the Twilight series?</td>\n",
       "      <td>Breaking Dawn</td>\n",
       "      <td>books</td>\n",
       "      <td>ordinal</td>\n",
       "      <td>[{'name': 'Q44523', 'entityType': 'entity', 'l...</td>\n",
       "      <td>[{'name': 'Q53945', 'label': 'Breaking Dawn'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88bdb808</td>\n",
       "      <td>en</td>\n",
       "      <td>How many games are in the Uncharted series?</td>\n",
       "      <td>6</td>\n",
       "      <td>videogames</td>\n",
       "      <td>count</td>\n",
       "      <td>[{'name': 'Q1064135', 'entityType': 'entity', ...</td>\n",
       "      <td>[{'name': 'Q17150', 'label': 'Uncharted: Drake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ecfd471d</td>\n",
       "      <td>en</td>\n",
       "      <td>As of 2015, which group held the record for th...</td>\n",
       "      <td>U2</td>\n",
       "      <td>music</td>\n",
       "      <td>generic</td>\n",
       "      <td>[{'name': 'Q41254', 'entityType': 'entity', 'l...</td>\n",
       "      <td>[{'name': 'Q396', 'label': 'U2'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d8dc3ff</td>\n",
       "      <td>en</td>\n",
       "      <td>Who is the oldest person to ever win an Academ...</td>\n",
       "      <td>James Ivory</td>\n",
       "      <td>movies</td>\n",
       "      <td>superlative</td>\n",
       "      <td>[{'name': 'Q19020', 'entityType': 'entity', 'l...</td>\n",
       "      <td>[{'name': 'Q51577', 'label': 'James Ivory'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118daa85</td>\n",
       "      <td>en</td>\n",
       "      <td>Which Mario Kart games do not feature Link as ...</td>\n",
       "      <td>Super Mario Kart, Mario Kart 64, Mario Kart: S...</td>\n",
       "      <td>videogames</td>\n",
       "      <td>difference</td>\n",
       "      <td>[{'name': 'Q188196', 'entityType': 'entity', '...</td>\n",
       "      <td>[{'name': 'Q1061560', 'label': 'Super Mario Ka...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id lang                                           question  \\\n",
       "0  9ace9041   en    What is the fourth book in the Twilight series?   \n",
       "1  88bdb808   en        How many games are in the Uncharted series?   \n",
       "2  ecfd471d   en  As of 2015, which group held the record for th...   \n",
       "3  5d8dc3ff   en  Who is the oldest person to ever win an Academ...   \n",
       "4  118daa85   en  Which Mario Kart games do not feature Link as ...   \n",
       "\n",
       "                                          answerText    category  \\\n",
       "0                                      Breaking Dawn       books   \n",
       "1                                                  6  videogames   \n",
       "2                                                 U2       music   \n",
       "3                                        James Ivory      movies   \n",
       "4  Super Mario Kart, Mario Kart 64, Mario Kart: S...  videogames   \n",
       "\n",
       "  complexityType                                     questionEntity  \\\n",
       "0        ordinal  [{'name': 'Q44523', 'entityType': 'entity', 'l...   \n",
       "1          count  [{'name': 'Q1064135', 'entityType': 'entity', ...   \n",
       "2        generic  [{'name': 'Q41254', 'entityType': 'entity', 'l...   \n",
       "3    superlative  [{'name': 'Q19020', 'entityType': 'entity', 'l...   \n",
       "4     difference  [{'name': 'Q188196', 'entityType': 'entity', '...   \n",
       "\n",
       "                                        answerEntity  \n",
       "0     [{'name': 'Q53945', 'label': 'Breaking Dawn'}]  \n",
       "1  [{'name': 'Q17150', 'label': 'Uncharted: Drake...  \n",
       "2                  [{'name': 'Q396', 'label': 'U2'}]  \n",
       "3       [{'name': 'Q51577', 'label': 'James Ivory'}]  \n",
       "4  [{'name': 'Q1061560', 'label': 'Super Mario Ka...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'].to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ordinal', 'intersection', 'generic', 'superlative', 'yesno',\n",
       "       'comparative', 'multihop', 'difference', 'count'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].to_pandas()['complexityType'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-short learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = 'google/t5-3b-ssm'\n",
    "# model_checkpoint = 'google/t5-large-ssm'\n",
    "model_checkpoint = '/mnt/storage/QA_System_Project/seq2seq_runs/mintaka_only_experiments_mintaka_tunned/model_t5_large_ssm_nq/models/checkpoint-7000/'\n",
    "device = torch.device('cuda:2')\n",
    "batch_size = 8\n",
    "dataset_split = 'test'\n",
    "\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "seq2seq_pipeline = transformers.pipeline(\n",
    "    task='text2text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/AmazonScience___mintaka/en/1.0.0/bb35d95f07aed78fa590601245009c5f585efe909dbd4a8f2a4025ccf65bb11d/cache-d82ee0eb58e1802d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/AmazonScience___mintaka/en/1.0.0/bb35d95f07aed78fa590601245009c5f585efe909dbd4a8f2a4025ccf65bb11d/cache-c8955b1d3baff169.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acb7a4c0643400a857ea8e1a92f4517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda batch: convert_to_features(\n",
    "        batch, tokenizer, label_feature_name=\"answerText\"\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "columns = [\n",
    "    \"input_ids\",\n",
    "    \"labels\",\n",
    "    \"attention_mask\",\n",
    "]\n",
    "dataset.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7240d66f7bc485290c0363167e43399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset[dataset_split], batch_size=batch_size)\n",
    "\n",
    "generated_text = []\n",
    "for batch in tqdm(dataloader):\n",
    "    outputs = model.generate(\n",
    "        batch['input_ids'].to(device),\n",
    "        max_new_tokens=64,\n",
    "        return_dict_in_generate=True,\n",
    "        # output_scores=True,\n",
    "    )\n",
    "\n",
    "    _generated_text = tokenizer.batch_decode(\n",
    "        outputs.sequences,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "    )\n",
    "\n",
    "    generated_text.extend(_generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe68ef0a864a49048358ececf660ca30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_entities_idxs = []\n",
    "for text in tqdm(generated_text):\n",
    "    results = get_wd_search_results(text, max_results=1)\n",
    "    if len(results) > 0:\n",
    "        entity = results[0]\n",
    "    else:\n",
    "        entity = None\n",
    "\n",
    "    generated_entities_idxs.append(entity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALARM: Edit path_to_test_file and path_to_predictions to correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Exact Match: 0.1705\n",
      "F1: 0.1722\n",
      "Hits@1: 0.1732\n"
     ]
    }
   ],
   "source": [
    "with open(f'preds_{dataset_split}.json', 'w') as f:\n",
    "    json.dump(\n",
    "        dict(zip(dataset[dataset_split]['id'], generated_entities_idxs)),\n",
    "        f\n",
    "    )\n",
    "\n",
    "## ALARM: Edit path_to_test_file and path_to_predictions to correct\n",
    "!python evaluate.py --mode kg --path_to_test_set ./mintaka_test.json --path_to_predictions ./preds_test.json --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100f05e846c5436fb7b615386b5aa15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@1:  0.16925\n"
     ]
    }
   ],
   "source": [
    "from evaluate import format_predictions, calculate_h1\n",
    "\n",
    "is_hit = []\n",
    "is_type_hit = []\n",
    "for generated_entity_idx, target_answer_entities in tqdm(\n",
    "    zip(generated_entities_idxs, dataset[dataset_split]['answerEntity']),\n",
    "    total=len(generated_entities_idxs)\n",
    "):\n",
    "    answer = [e['name'] for e in target_answer_entities]\n",
    "    pred = format_predictions(generated_entity_idx, 'kg')\n",
    "    is_hit.append(\n",
    "        bool(calculate_h1(pred, answer, 'kg'))\n",
    "    )\n",
    "\n",
    "    if pred is not None and len(pred) > 0:\n",
    "        answer_types = set()\n",
    "        for ans in answer:\n",
    "            answer_types = answer_types.union(Entity(ans).instance_of)\n",
    "        \n",
    "        pred_types = set(Entity(pred[0]).instance_of)\n",
    "\n",
    "        is_type_hit.append(\n",
    "            len(answer_types.intersection(pred_types)) > 0\n",
    "        )\n",
    "    else:\n",
    "        is_type_hit.append(False)\n",
    "\n",
    "\n",
    "hits_1 = sum(is_hit) / len(generated_entities_idxs)\n",
    "print(\"Hits@1: \", hits_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@1 comparative  = 0.27\n",
      "Hits@1 count        = 0.0\n",
      "Hits@1 difference   = 0.1425\n",
      "Hits@1 generic      = 0.19\n",
      "Hits@1 intersection = 0.285\n",
      "Hits@1 multihop     = 0.09\n",
      "Hits@1 ordinal      = 0.15\n",
      "Hits@1 superlative  = 0.375\n",
      "Hits@1 yesno        = 0.0\n"
     ]
    }
   ],
   "source": [
    "df = dataset[dataset_split].to_pandas()\n",
    "df['is_hit'] = is_hit\n",
    "df['is_type_hit'] = is_type_hit\n",
    "df['generated_entity'] = generated_entities_idxs\n",
    "\n",
    "for complexity_type, group in df.groupby('complexityType'):\n",
    "    print(f\"Hits@1 {complexity_type:12s} = {group['is_hit'].sum() / group.index.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of errors with incorrect type                = 0.31266927475173034\n",
      "Proportion of errors with incorrect type (comparative ) = 0.22602739726027396\n",
      "Proportion of errors with incorrect type (count       ) = 0.0\n",
      "Proportion of errors with incorrect type (difference  ) = 0.3848396501457726\n",
      "Proportion of errors with incorrect type (generic     ) = 0.3395061728395062\n",
      "Proportion of errors with incorrect type (intersection) = 0.7027972027972028\n",
      "Proportion of errors with incorrect type (multihop    ) = 0.3516483516483517\n",
      "Proportion of errors with incorrect type (ordinal     ) = 0.43529411764705883\n",
      "Proportion of errors with incorrect type (superlative ) = 0.576\n",
      "Proportion of errors with incorrect type (yesno       ) = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'Proportion of errors with incorrect type                =',\n",
    "    df[(df['is_type_hit']) & (~df['is_hit'])].index.size / df[~df['is_hit']].index.size\n",
    ")\n",
    "\n",
    "for complexity_type, group in df.groupby('complexityType'):\n",
    "    print(\n",
    "        f'Proportion of errors with incorrect type ({complexity_type:12s}) =',\n",
    "        group[(group['is_type_hit']) & (~group['is_hit'])].index.size / group[~group['is_hit']].index.size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
